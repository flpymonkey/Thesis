{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## install finrl library\n",
    "# !pip install wrds\n",
    "# !pip install quantstats\n",
    "# !pip install torch_geometric\n",
    "# !pip install swig\n",
    "# !pip install -q condacolab\n",
    "# !pip install shimmy\n",
    "# import condacolab\n",
    "# condacolab.install()\n",
    "# !apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
    "# !pip install git+https://github.com/flpymonkey/FinRL_Online_Portfolio_Benchmarks.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "\n",
    "import gym\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import copy\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import quantstats as qs\n",
    "except ModuleNotFoundError:\n",
    "    raise ModuleNotFoundError(\n",
    "        \"\"\"QuantStats module not found, environment can't plot results and calculate indicadors.\n",
    "        This module is not installed with FinRL. Install by running one of the options:\n",
    "        pip install quantstats --upgrade --no-cache-dir\n",
    "        conda install -c ranaroussi quantstats\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "class PortfolioOptimizationEnv(gym.Env):\n",
    "    \"\"\"A portfolio allocation environment for OpenAI gym.\n",
    "\n",
    "    This environment simulates the interactions between an agent and the financial market\n",
    "    based on data provided by a dataframe. The dataframe contains the time series of\n",
    "    features defined by the user (such as closing, high and low prices) and must have\n",
    "    a time and a tic column with a list of datetimes and ticker symbols respectively.\n",
    "    An example of dataframe is shown below::\n",
    "\n",
    "            date        high            low             close           tic\n",
    "        0   2020-12-23  0.157414        0.127420        0.136394        ADA-USD\n",
    "        1   2020-12-23  34.381519       30.074295       31.097898       BNB-USD\n",
    "        2   2020-12-23  24024.490234    22802.646484    23241.345703    BTC-USD\n",
    "        3   2020-12-23  0.004735        0.003640        0.003768        DOGE-USD\n",
    "        4   2020-12-23  637.122803      560.364258      583.714600      ETH-USD\n",
    "        ... ...         ...             ...             ...             ...\n",
    "\n",
    "    Based on this dataframe, the environment will create an observation space that can\n",
    "    be a Dict or a Box. The Box observation space is a three-dimensional array of shape\n",
    "    (f, n, t), where f is the number of features, n is the number of stocks in the\n",
    "    portfolio and t is the user-defined time window. If the environment is created with\n",
    "    the parameter return_last_action set to True, the observation space is a Dict with\n",
    "    the following keys::\n",
    "\n",
    "        {\n",
    "        \"state\": three-dimensional Box (f, n, t) representing the time series,\n",
    "        \"last_action\": one-dimensional Box (n+1,) representing the portfolio weights\n",
    "        }\n",
    "\n",
    "    Note that the action space of this environment is an one-dimensional Box with size\n",
    "    n + 1 because the portfolio weights must contains the weights related to all the\n",
    "    stocks in the portfolio and to the remaining cash.\n",
    "\n",
    "    Attributes:\n",
    "        action_space: Action space.\n",
    "        observation_space: Observation space.\n",
    "        episode_length: Number of timesteps of an episode.\n",
    "        portfolio_size: Number of stocks in the portfolio.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        initial_amount,\n",
    "        order_df=True,\n",
    "        return_last_action=False,\n",
    "        normalize_df=\"by_previous_time\",\n",
    "        reward_scaling=1,\n",
    "        comission_fee_model=\"trf\",\n",
    "        comission_fee_pct=0,\n",
    "        features=[\"close\", \"high\", \"low\"],\n",
    "        valuation_feature=\"close\",\n",
    "        time_column=\"date\",\n",
    "        time_format=\"%Y-%m-%d\",\n",
    "        tic_column=\"tic\",\n",
    "        tics_in_portfolio=\"all\",\n",
    "        time_window=1,\n",
    "        cwd=\"./\",\n",
    "        new_gym_api=False,\n",
    "    ):\n",
    "        \"\"\"Initializes environment's instance.\n",
    "\n",
    "        Args:\n",
    "            df: Dataframe with market information over a period of time.\n",
    "            initial_amount: Initial amount of cash available to be invested.\n",
    "            order_df: If True input dataframe is ordered by time.\n",
    "            return_last_action: If True, observations also return the last performed\n",
    "                action. Note that, in that case, the observation space is a Dict.\n",
    "            normalize_df: Defines the normalization method applied to input dataframe.\n",
    "                Possible values are \"by_previous_time\", \"by_fist_time_window_value\",\n",
    "                \"by_COLUMN_NAME\" (where COLUMN_NAME must be changed to a real column\n",
    "                name) and a custom function. If None no normalization is done.\n",
    "            reward_scaling: A scaling factor to multiply the reward function. This\n",
    "                factor can help training.\n",
    "            comission_fee_model: Model used to simulate comission fee. Possible values\n",
    "                are \"trf\" (for transaction remainder factor model) and \"wvm\" (for weights\n",
    "                vector modifier model). If None, commission fees are not considered.\n",
    "            comission_fee_pct: Percentage to be used in comission fee. It must be a value\n",
    "                between 0 and 1.\n",
    "            features: List of features to be considered in the observation space. The\n",
    "                items of the list must be names of columns of the input dataframe.\n",
    "            valuation_feature: Feature to be considered in the portfolio value calculation.\n",
    "            time_column: Name of the dataframe's column that contain the datetimes that\n",
    "                index the dataframe.\n",
    "            time_format: Formatting string of time column.\n",
    "            tic_name: Name of the dataframe's column that contain ticker symbols.\n",
    "            tics_in_portfolio: List of ticker symbols to be considered as part of the\n",
    "                portfolio. If \"all\", all tickers of input data are considered.\n",
    "            time_window: Size of time window.\n",
    "            cwd: Local repository in which resulting graphs will be saved.\n",
    "            new_gym_api: If True, the environment will use the new gym api standard for\n",
    "                step and reset methods.\n",
    "        \"\"\"\n",
    "        self._time_window = time_window\n",
    "        self._time_index = time_window - 1\n",
    "        self._time_column = time_column\n",
    "        self._time_format = time_format\n",
    "        self._tic_column = tic_column\n",
    "        self._df = df\n",
    "        self._initial_amount = initial_amount\n",
    "        self._return_last_action = return_last_action\n",
    "        self._reward_scaling = reward_scaling\n",
    "        self._comission_fee_pct = comission_fee_pct\n",
    "        self._comission_fee_model = comission_fee_model\n",
    "        self._features = features\n",
    "        self._valuation_feature = valuation_feature\n",
    "        self._cwd = Path(cwd)\n",
    "        self._new_gym_api = new_gym_api\n",
    "\n",
    "        # results file\n",
    "        self._results_file = self._cwd / \"results\" / \"rl\"\n",
    "        self._results_file.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # initialize price variation\n",
    "        self._df_price_variation = None\n",
    "\n",
    "        # preprocess data\n",
    "        self._preprocess_data(order_df, normalize_df, tics_in_portfolio)\n",
    "\n",
    "        # dims and spaces\n",
    "        self._tic_list = self._df[self._tic_column].unique()\n",
    "        self.portfolio_size = (\n",
    "            len(self._tic_list)\n",
    "            if tics_in_portfolio == \"all\"\n",
    "            else len(tics_in_portfolio)\n",
    "        )\n",
    "        action_space = 1 + self.portfolio_size\n",
    "\n",
    "        # sort datetimes and define episode length\n",
    "        self._sorted_times = sorted(set(self._df[time_column]))\n",
    "        self.episode_length = len(self._sorted_times) - time_window + 1\n",
    "\n",
    "        # define action space\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(action_space,))\n",
    "\n",
    "        # define observation state\n",
    "        if self._return_last_action:\n",
    "            # if  last action must be returned, a dict observation\n",
    "            # is defined\n",
    "            self.observation_space = spaces.Dict(\n",
    "                {\n",
    "                    \"state\": spaces.Box(\n",
    "                        low=-np.inf,\n",
    "                        high=np.inf,\n",
    "                        shape=(\n",
    "                            len(self._features),\n",
    "                            len(self._tic_list),\n",
    "                            self._time_window,\n",
    "                        ),\n",
    "                    ),\n",
    "                    \"last_action\": spaces.Box(low=0, high=1, shape=(action_space,)),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            # if information about last action is not relevant,\n",
    "            # a 3D observation space is defined\n",
    "            self.observation_space = spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                shape=(len(self._features), len(self._tic_list), self._time_window),\n",
    "            )\n",
    "\n",
    "        self._reset_memory()\n",
    "\n",
    "        self._portfolio_value = self._initial_amount\n",
    "        self._terminal = False\n",
    "\n",
    "        # Use this to save the state in the last terminal state in case the environment resets \n",
    "        self._terminal_action_memory = None\n",
    "        self._terminal_asset_memory = None\n",
    "        self._terminal_date_memory = None\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"Performs a simulation step.\n",
    "\n",
    "        Args:\n",
    "            actions: An unidimensional array containing the new portfolio\n",
    "                weights.\n",
    "\n",
    "        Note:\n",
    "            If the environment was created with \"return_last_action\" set to\n",
    "            True, the next state returned will be a Dict. If it's set to False,\n",
    "            the next state will be a Box. You can check the observation state\n",
    "            through the attribute \"observation_space\".\n",
    "\n",
    "        Returns:\n",
    "            If \"new_gym_api\" is set to True, the following tuple is returned:\n",
    "            (state, reward, terminal, truncated, info). If it's set to False,\n",
    "            the following tuple is returned: (state, reward, terminal, info).\n",
    "\n",
    "            state: Next simulation state.\n",
    "            reward: Reward related to the last performed action.\n",
    "            terminal: If True, the environment is in a terminal state.\n",
    "            truncated: If True, the environment has passed it's simulation\n",
    "                time limit. Currently, it's always False.\n",
    "            info: A dictionary containing informations about the last state.\n",
    "        \"\"\"\n",
    "        self._terminal = self._time_index >= len(self._sorted_times) - 1\n",
    "\n",
    "        if self._terminal:\n",
    "            metrics_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"date\": self._date_memory,\n",
    "                    \"returns\": self._portfolio_return_memory,\n",
    "                    \"rewards\": self._portfolio_reward_memory,\n",
    "                    \"portfolio_values\": self._asset_memory[\"final\"],\n",
    "                }\n",
    "            )\n",
    "            metrics_df.set_index(\"date\", inplace=True)\n",
    "\n",
    "            plt.plot(metrics_df[\"portfolio_values\"], \"r\")\n",
    "            plt.title(\"Portfolio Value Over Time\")\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Portfolio value\")\n",
    "            plt.savefig(self._results_file / \"portfolio_value.png\")\n",
    "            plt.close()\n",
    "\n",
    "            plt.plot(self._portfolio_reward_memory, \"r\")\n",
    "            plt.title(\"Reward Over Time\")\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.savefig(self._results_file / \"reward.png\")\n",
    "            plt.close()\n",
    "\n",
    "            plt.plot(self._actions_memory)\n",
    "            plt.title(\"Actions performed\")\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Weight\")\n",
    "            plt.savefig(self._results_file / \"actions.png\")\n",
    "            plt.close()\n",
    "\n",
    "            print(\"=================================\")\n",
    "            print(\"Initial portfolio value:{}\".format(self._asset_memory[\"final\"][0]))\n",
    "            print(f\"Final portfolio value: {self._portfolio_value}\")\n",
    "            print(\n",
    "                \"Final accumulative portfolio value: {}\".format(\n",
    "                    self._portfolio_value / self._asset_memory[\"final\"][0]\n",
    "                )\n",
    "            )\n",
    "            print(\n",
    "                \"Maximum DrawDown: {}\".format(\n",
    "                    qs.stats.max_drawdown(metrics_df[\"portfolio_values\"])\n",
    "                )\n",
    "            )\n",
    "            print(\"Sharpe ratio: {}\".format(qs.stats.sharpe(metrics_df[\"returns\"])))\n",
    "            print(\"=================================\")\n",
    "\n",
    "            qs.plots.snapshot(\n",
    "                metrics_df[\"returns\"],\n",
    "                show=False,\n",
    "                savefig=self._results_file / \"portfolio_summary.png\",\n",
    "            )\n",
    "\n",
    "            # Save the asset memory in the terminal state before the environment is reset\n",
    "            self._terminal_asset_memory = copy.deepcopy(self._asset_memory)\n",
    "            self._terminal_date_memory = copy.deepcopy(self._date_memory)\n",
    "            self._terminal_action_memory = copy.deepcopy(self._actions_memory)\n",
    "\n",
    "            file_path = './action_dump/actions' + str(len(self._terminal_action_memory)) + '.csv'\n",
    "            np.savetxt(file_path, self._terminal_action_memory, delimiter=',', fmt='%.6f')\n",
    "\n",
    "            if self._new_gym_api:\n",
    "                return self._state, self._reward, self._terminal, False, self._info\n",
    "            return self._state, self._reward, self._terminal, self._info\n",
    "\n",
    "        else:\n",
    "            # transform action to numpy array (if it's a list)\n",
    "            actions = np.array(actions, dtype=np.float32)\n",
    "\n",
    "            # if necessary, normalize weights\n",
    "            if math.isclose(np.sum(actions), 1, abs_tol=1e-6) and np.min(actions) >= 0:\n",
    "                weights = actions\n",
    "            else:\n",
    "                action_sum = np.sum(actions)\n",
    "                weights = actions / action_sum\n",
    "                if not action_sum:   \n",
    "                    weights = np.zeros(len(weights))\n",
    "                    weights[0] = 1\n",
    "                # print(weights)\n",
    "                # weights = self._softmax_normalization(actions)\n",
    "\n",
    "            # save initial portfolio weights for this time step\n",
    "            self._actions_memory.append(weights)\n",
    "\n",
    "            # get last step final weights and portfolio_value\n",
    "            last_weights = self._final_weights[-1]\n",
    "\n",
    "            # load next state\n",
    "            self._time_index += 1\n",
    "            self._state, self._info = self._get_state_and_info_from_time_index(\n",
    "                self._time_index\n",
    "            )\n",
    "\n",
    "            # if using weights vector modifier, we need to modify weights vector\n",
    "            if self._comission_fee_model == \"wvm\":\n",
    "                delta_weights = weights - last_weights\n",
    "                delta_assets = delta_weights[1:]  # disconsider\n",
    "                # calculate fees considering weights modification\n",
    "                fees = np.sum(np.abs(delta_assets * self._portfolio_value))\n",
    "                if fees > weights[0] * self._portfolio_value:\n",
    "                    weights = last_weights\n",
    "                    # maybe add negative reward\n",
    "                else:\n",
    "                    portfolio = weights * self._portfolio_value\n",
    "                    portfolio[0] -= fees\n",
    "                    self._portfolio_value = np.sum(portfolio)  # new portfolio value\n",
    "                    weights = portfolio / self._portfolio_value  # new weights\n",
    "            elif self._comission_fee_model == \"trf\":\n",
    "                last_mu = 1\n",
    "                mu = 1 - 2 * self._comission_fee_pct + self._comission_fee_pct**2\n",
    "                while abs(mu - last_mu) > 1e-10:\n",
    "                    last_mu = mu\n",
    "                    mu = (\n",
    "                        1\n",
    "                        - self._comission_fee_pct * weights[0]\n",
    "                        - (2 * self._comission_fee_pct - self._comission_fee_pct**2)\n",
    "                        * np.sum(np.maximum(last_weights[1:] - mu * weights[1:], 0))\n",
    "                    ) / (1 - self._comission_fee_pct * weights[0])\n",
    "                self._info[\"trf_mu\"] = mu\n",
    "                self._portfolio_value = mu * self._portfolio_value\n",
    "\n",
    "            # save initial portfolio value of this time step\n",
    "            self._asset_memory[\"initial\"].append(self._portfolio_value)\n",
    "\n",
    "            # time passes and time variation changes the portfolio distribution\n",
    "            portfolio = self._portfolio_value * (weights * self._price_variation)\n",
    "\n",
    "            # calculate new portfolio value and weights\n",
    "            self._portfolio_value = np.sum(portfolio)\n",
    "            weights = portfolio / self._portfolio_value\n",
    "\n",
    "            # save final portfolio value and weights of this time step\n",
    "            self._asset_memory[\"final\"].append(self._portfolio_value)\n",
    "            self._final_weights.append(weights)\n",
    "\n",
    "            # save date memory\n",
    "            self._date_memory.append(self._info[\"end_time\"])\n",
    "\n",
    "            # define portfolio return\n",
    "            rate_of_return = (\n",
    "                self._asset_memory[\"final\"][-1] / self._asset_memory[\"final\"][-2]\n",
    "            )\n",
    "            portfolio_return = rate_of_return - 1\n",
    "            portfolio_reward = np.log(rate_of_return)\n",
    "\n",
    "            # save portfolio return memory\n",
    "            self._portfolio_return_memory.append(portfolio_return)\n",
    "            self._portfolio_reward_memory.append(portfolio_reward)\n",
    "\n",
    "            # Define portfolio return\n",
    "            self._reward = portfolio_reward\n",
    "            self._reward = self._reward * self._reward_scaling\n",
    "        \n",
    "        if self._new_gym_api:\n",
    "            return self._state, self._reward, self._terminal, False, self._info\n",
    "        return self._state, self._reward, self._terminal, self._info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment and returns it to its initial state (the\n",
    "        fist date of the dataframe).\n",
    "\n",
    "        Note:\n",
    "            If the environment was created with \"return_last_action\" set to\n",
    "            True, the initial state will be a Dict. If it's set to False,\n",
    "            the initial state will be a Box. You can check the observation\n",
    "            state through the attribute \"observation_space\".\n",
    "\n",
    "        Returns:\n",
    "            If \"new_gym_api\" is set to True, the following tuple is returned:\n",
    "            (state, info). If it's set to False, only the initial state is\n",
    "            returned.\n",
    "\n",
    "            state: Initial state.\n",
    "            info: Initial state info.\n",
    "        \"\"\"\n",
    "        # time_index must start a little bit in the future to implement lookback\n",
    "        self._time_index = self._time_window - 1\n",
    "        self._reset_memory()\n",
    "\n",
    "        self._state, self._info = self._get_state_and_info_from_time_index(\n",
    "            self._time_index\n",
    "        )\n",
    "        self._portfolio_value = self._initial_amount\n",
    "        self._terminal = False\n",
    "\n",
    "        if self._new_gym_api:\n",
    "            return self._state, self._info\n",
    "        return self._state\n",
    "\n",
    "    def _get_state_and_info_from_time_index(self, time_index):\n",
    "        \"\"\"Gets state and information given a time index. It also updates \"data\"\n",
    "        attribute with information about the current simulation step.\n",
    "\n",
    "        Args:\n",
    "            time_index: An integer that represents the index of a specific datetime.\n",
    "                The initial datetime of the dataframe is given by 0.\n",
    "\n",
    "        Note:\n",
    "            If the environment was created with \"return_last_action\" set to\n",
    "            True, the returned state will be a Dict. If it's set to False,\n",
    "            the returned state will be a Box. You can check the observation\n",
    "            state through the attribute \"observation_space\".\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the following form: (state, info).\n",
    "\n",
    "            state: The state of the current time index. It can be a Box or a Dict.\n",
    "            info: A dictionary with some informations about the current simulation\n",
    "                step. The dict has the following keys::\n",
    "\n",
    "                {\n",
    "                \"tics\": List of ticker symbols,\n",
    "                \"start_time\": Start time of current time window,\n",
    "                \"start_time_index\": Index of start time of current time window,\n",
    "                \"end_time\": End time of current time window,\n",
    "                \"end_time_index\": Index of end time of current time window,\n",
    "                \"data\": Data related to the current time window,\n",
    "                \"price_variation\": Price variation of current time step\n",
    "                }\n",
    "        \"\"\"\n",
    "        # returns state in form (channels, tics, timesteps)\n",
    "        end_time = self._sorted_times[time_index]\n",
    "        start_time = self._sorted_times[time_index - (self._time_window - 1)]\n",
    "\n",
    "        # define data to be used in this time step\n",
    "        self._data = self._df[\n",
    "            (self._df[self._time_column] >= start_time)\n",
    "            & (self._df[self._time_column] <= end_time)\n",
    "        ][[self._time_column, self._tic_column] + self._features]\n",
    "\n",
    "        # define price variation of this time_step\n",
    "        self._price_variation = self._df_price_variation[\n",
    "            self._df_price_variation[self._time_column] == end_time\n",
    "        ][self._valuation_feature].to_numpy()\n",
    "        self._price_variation = np.insert(self._price_variation, 0, 1)\n",
    "\n",
    "        # define state to be returned\n",
    "        state = None\n",
    "        for tic in self._tic_list:\n",
    "            tic_data = self._data[self._data[self._tic_column] == tic]\n",
    "            tic_data = tic_data[self._features].to_numpy().T\n",
    "            tic_data = tic_data[..., np.newaxis]\n",
    "            state = tic_data if state is None else np.append(state, tic_data, axis=2)\n",
    "        state = state.transpose((0, 2, 1))\n",
    "        info = {\n",
    "            \"tics\": self._tic_list,\n",
    "            \"start_time\": start_time,\n",
    "            \"start_time_index\": time_index - (self._time_window - 1),\n",
    "            \"end_time\": end_time,\n",
    "            \"end_time_index\": time_index,\n",
    "            \"data\": self._data,\n",
    "            \"price_variation\": self._price_variation,\n",
    "        }\n",
    "        return self._standardize_state(state), info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"Renders the environment.\n",
    "\n",
    "        Returns:\n",
    "            Observation of current simulation step.\n",
    "        \"\"\"\n",
    "        return self._state\n",
    "\n",
    "    def _softmax_normalization(self, actions):\n",
    "        \"\"\"Normalizes the action vector using softmax function.\n",
    "\n",
    "        Returns:\n",
    "            Normalized action vector (portfolio vector).\n",
    "        \"\"\"\n",
    "        numerator = np.exp(actions)\n",
    "        denominator = np.sum(np.exp(actions))\n",
    "        softmax_output = numerator / denominator\n",
    "        return softmax_output\n",
    "\n",
    "    def enumerate_portfolio(self):\n",
    "        \"\"\"Enumerates the current porfolio by showing the ticker symbols\n",
    "        of all the investments considered in the portfolio.\n",
    "        \"\"\"\n",
    "        print(\"Index: 0. Tic: Cash\")\n",
    "        for index, tic in enumerate(self._tic_list):\n",
    "            print(f\"Index: {index + 1}. Tic: {tic}\")\n",
    "\n",
    "    def _preprocess_data(self, order, normalize, tics_in_portfolio):\n",
    "        \"\"\"Orders and normalizes the environment's dataframe.\n",
    "\n",
    "        Args:\n",
    "            order: If true, the dataframe will be ordered by ticker list\n",
    "                and datetime.\n",
    "            normalize: Defines the normalization method applied to the dataframe.\n",
    "                Possible values are \"by_previous_time\", \"by_fist_time_window_value\",\n",
    "                \"by_COLUMN_NAME\" (where COLUMN_NAME must be changed to a real column\n",
    "                name) and a custom function. If None no normalization is done.\n",
    "            tics_in_portfolio: List of ticker symbols to be considered as part of the\n",
    "                portfolio. If \"all\", all tickers of input data are considered.\n",
    "        \"\"\"\n",
    "        # order time dataframe by tic and time\n",
    "        if order:\n",
    "            self._df = self._df.sort_values(by=[self._tic_column, self._time_column])\n",
    "        # defining price variation after ordering dataframe\n",
    "        self._df_price_variation = self._temporal_variation_df()\n",
    "        # select only stocks in portfolio\n",
    "        if tics_in_portfolio != \"all\":\n",
    "            self._df_price_variation = self._df_price_variation[\n",
    "                self._df_price_variation[self._tic_column].isin(tics_in_portfolio)\n",
    "            ]\n",
    "        # apply normalization\n",
    "        if normalize:\n",
    "            self._normalize_dataframe(normalize)\n",
    "        # transform str to datetime\n",
    "        self._df[self._time_column] = pd.to_datetime(self._df[self._time_column])\n",
    "        self._df_price_variation[self._time_column] = pd.to_datetime(\n",
    "            self._df_price_variation[self._time_column]\n",
    "        )\n",
    "        # transform numeric variables to float32 (compatibility with pytorch)\n",
    "        self._df[self._features] = self._df[self._features].astype(\"float32\")\n",
    "        self._df_price_variation[self._features] = self._df_price_variation[\n",
    "            self._features\n",
    "        ].astype(\"float32\")\n",
    "\n",
    "    def _reset_memory(self):\n",
    "        \"\"\"Resets the environment's memory.\"\"\"\n",
    "        date_time = self._sorted_times[self._time_index]\n",
    "        # memorize portfolio value each step\n",
    "        self._asset_memory = {\n",
    "            \"initial\": [self._initial_amount],\n",
    "            \"final\": [self._initial_amount],\n",
    "        }\n",
    "        # memorize portfolio return and reward each step\n",
    "        self._portfolio_return_memory = [0]\n",
    "        self._portfolio_reward_memory = [0]\n",
    "        # initial action: all money is allocated in cash\n",
    "        self._actions_memory = [\n",
    "            np.array([1] + [0] * self.portfolio_size, dtype=np.float32)\n",
    "        ]\n",
    "        # memorize portfolio weights at the ending of time step\n",
    "        self._final_weights = [\n",
    "            np.array([1] + [0] * self.portfolio_size, dtype=np.float32)\n",
    "        ]\n",
    "        # memorize datetimes\n",
    "        self._date_memory = [date_time]\n",
    "\n",
    "    def _standardize_state(self, state):\n",
    "        \"\"\"Standardize the state given the observation space. If \"return_last_action\"\n",
    "        is set to False, a three-dimensional box is returned. If it's set to True, a\n",
    "        dictionary is returned. The dictionary follows the standard below::\n",
    "\n",
    "            {\n",
    "            \"state\": Three-dimensional box representing the current state,\n",
    "            \"last_action\": One-dimensional box representing the last action\n",
    "            }\n",
    "        \"\"\"\n",
    "        last_action = self._actions_memory[-1]\n",
    "        if self._return_last_action:\n",
    "            return {\"state\": state, \"last_action\": last_action}\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "    def _normalize_dataframe(self, normalize):\n",
    "        \"\"\" \"Normalizes the environment's dataframe.\n",
    "\n",
    "        Args:\n",
    "            normalize: Defines the normalization method applied to the dataframe.\n",
    "                Possible values are \"by_previous_time\", \"by_fist_time_window_value\",\n",
    "                \"by_COLUMN_NAME\" (where COLUMN_NAME must be changed to a real column\n",
    "                name) and a custom function. If None no normalization is done.\n",
    "\n",
    "        Note:\n",
    "            If a custom function is used in the normalization, it must have an\n",
    "            argument representing the environment's dataframe.\n",
    "        \"\"\"\n",
    "        if type(normalize) == str:\n",
    "            if normalize == \"by_fist_time_window_value\":\n",
    "                print(\n",
    "                    \"Normalizing {} by first time window value...\".format(\n",
    "                        self._features\n",
    "                    )\n",
    "                )\n",
    "                self._df = self._temporal_variation_df(self._time_window - 1)\n",
    "            elif normalize == \"by_previous_time\":\n",
    "                print(f\"Normalizing {self._features} by previous time...\")\n",
    "                self._df = self._temporal_variation_df()\n",
    "            elif normalize.startswith(\"by_\"):\n",
    "                normalizer_column = normalize[3:]\n",
    "                print(f\"Normalizing {self._features} by {normalizer_column}\")\n",
    "                for column in self._features:\n",
    "                    self._df[column] = self._df[column] / self._df[normalizer_column]\n",
    "        elif callable(normalize):\n",
    "            print(\"Applying custom normalization function...\")\n",
    "            self._df = normalize(self._df)\n",
    "        else:\n",
    "            print(\"No normalization was performed.\")\n",
    "\n",
    "    def _temporal_variation_df(self, periods=1):\n",
    "        \"\"\"Calculates the temporal variation dataframe. For each feature, this\n",
    "        dataframe contains the rate of the current feature's value and the last\n",
    "        feature's value given a period. It's used to normalize the dataframe.\n",
    "\n",
    "        Args:\n",
    "            periods: Periods (in time indexes) to calculate temporal variation.\n",
    "\n",
    "        Returns:\n",
    "            Temporal variation dataframe.\n",
    "        \"\"\"\n",
    "        df_temporal_variation = self._df.copy()\n",
    "        prev_columns = []\n",
    "        for column in self._features:\n",
    "            prev_column = f\"prev_{column}\"\n",
    "            prev_columns.append(prev_column)\n",
    "            df_temporal_variation[prev_column] = df_temporal_variation.groupby(\n",
    "                self._tic_column\n",
    "            )[column].shift(periods=periods)\n",
    "            df_temporal_variation[column] = (\n",
    "                df_temporal_variation[column] / df_temporal_variation[prev_column]\n",
    "            )\n",
    "        df_temporal_variation = (\n",
    "            df_temporal_variation.drop(columns=prev_columns)\n",
    "            .fillna(1)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        return df_temporal_variation\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        \"\"\"Seeds the sources of randomness of this environment to guarantee\n",
    "        reproducibility.\n",
    "\n",
    "        Args:\n",
    "            seed: Seed value to be applied.\n",
    "\n",
    "        Returns:\n",
    "            Seed value applied.\n",
    "        \"\"\"\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_sb_env(self, env_number=1):\n",
    "        \"\"\"Generates an environment compatible with Stable Baselines 3. The\n",
    "        generated environment is a vectorized version of the current one.\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the generated environment and an initial observation.\n",
    "        \"\"\"\n",
    "        e = DummyVecEnv([lambda: self] * env_number)\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# from finrl.agents.portfolio_optimization.architectures import EIIE\n",
    "\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (14852, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN_START_DATE = '2009-04-01'\n",
    "TRAIN_END_DATE = '2020-12-31'\n",
    "TEST_START_DATE = '2021-01-01'\n",
    "TEST_END_DATE = '2024-01-01'\n",
    "\n",
    "# TRAIN_START_DATE = '2010-01-01'\n",
    "# TRAIN_END_DATE = '2021-10-01'\n",
    "# TEST_START_DATE = '2021-10-01'\n",
    "# TEST_END_DATE = '2023-03-01'\n",
    "\n",
    "TEST_TICKER = [\n",
    "   \"AXP\",\n",
    "    \"AMGN\",\n",
    "    \"AAPL\",\n",
    "    \"BA\",\n",
    "]\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "# from finrl.config_tickers import DOW_30_TICKER\n",
    "\n",
    "\n",
    "# # TODO Drop the DOW stock\n",
    "# value_to_remove = \"DOW\"\n",
    "# # Create a new list without the specified string\n",
    "# DOW_30_TICKER = [x for x in DOW_30_TICKER if x != value_to_remove]\n",
    "# print(DOW_30_TICKER)\n",
    "\n",
    "# No time window needed for PPO\n",
    "# TIME_WINDOW = 25\n",
    "COMMISSION_FEE_PERCENT = 0.001\n",
    "INITIAL_CASH = 1000000\n",
    "\n",
    "# TODO try different date ranges\n",
    "# TRAIN_START_DATE = '2009-01-01'\n",
    "# TRAIN_END_DATE = '2018-10-01'\n",
    "# TEST_START_DATE = '2018-10-01'\n",
    "# TEST_END_DATE = '2021-03-01'\n",
    "\n",
    "raw_df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = TEST_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date        open        high         low       close     volume  \\\n",
      "0      2009-04-01    3.717500    3.892857    3.710357    3.274470  589372000   \n",
      "1      2009-04-01   48.779999   48.930000   47.099998   34.259613   10850100   \n",
      "2      2009-04-01   13.340000   14.640000   13.080000   11.463929   27701800   \n",
      "3      2009-04-01   34.520000   35.599998   34.209999   26.850752    9288800   \n",
      "4      2009-04-02    3.933571    4.098214    3.920714    3.395579  812366800   \n",
      "...           ...         ...         ...         ...         ...        ...   \n",
      "14847  2023-12-28  261.529999  262.100006  257.679993  260.350006    5096400   \n",
      "14848  2023-12-29  193.899994  194.399994  191.729996  191.591385   42628800   \n",
      "14849  2023-12-29  287.859985  288.489990  286.390015  281.808167    1766600   \n",
      "14850  2023-12-29  187.750000  188.300003  186.529999  185.123352    1913800   \n",
      "14851  2023-12-29  260.670013  262.220001  259.559998  260.660004    3681900   \n",
      "\n",
      "        tic  day  \n",
      "0      AAPL    2  \n",
      "1      AMGN    2  \n",
      "2       AXP    2  \n",
      "3        BA    2  \n",
      "4      AAPL    3  \n",
      "...     ...  ...  \n",
      "14847    BA    3  \n",
      "14848  AAPL    4  \n",
      "14849  AMGN    4  \n",
      "14850   AXP    4  \n",
      "14851    BA    4  \n",
      "\n",
      "[14852 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(raw_df)\n",
    "processed = raw_df\n",
    "\n",
    "assert processed.notnull().all().all(), \"DataFrame contains null values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>0.018773</td>\n",
       "      <td>0.019501</td>\n",
       "      <td>0.018834</td>\n",
       "      <td>0.016610</td>\n",
       "      <td>0.313329</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>0.165873</td>\n",
       "      <td>0.164931</td>\n",
       "      <td>0.162829</td>\n",
       "      <td>0.121385</td>\n",
       "      <td>0.216841</td>\n",
       "      <td>AMGN</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>0.067544</td>\n",
       "      <td>0.073365</td>\n",
       "      <td>0.066315</td>\n",
       "      <td>0.059964</td>\n",
       "      <td>0.306650</td>\n",
       "      <td>AXP</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-04-01</td>\n",
       "      <td>0.077397</td>\n",
       "      <td>0.079819</td>\n",
       "      <td>0.077716</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.089997</td>\n",
       "      <td>BA</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-04-02</td>\n",
       "      <td>0.019865</td>\n",
       "      <td>0.020530</td>\n",
       "      <td>0.019902</td>\n",
       "      <td>0.017224</td>\n",
       "      <td>0.431881</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14847</th>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>0.586377</td>\n",
       "      <td>0.587655</td>\n",
       "      <td>0.585384</td>\n",
       "      <td>0.605043</td>\n",
       "      <td>0.049378</td>\n",
       "      <td>BA</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14848</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>0.979194</td>\n",
       "      <td>0.973850</td>\n",
       "      <td>0.973249</td>\n",
       "      <td>0.971834</td>\n",
       "      <td>0.022663</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14849</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>0.978849</td>\n",
       "      <td>0.972427</td>\n",
       "      <td>0.990078</td>\n",
       "      <td>0.998475</td>\n",
       "      <td>0.035306</td>\n",
       "      <td>AMGN</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14850</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>0.950633</td>\n",
       "      <td>0.943623</td>\n",
       "      <td>0.945701</td>\n",
       "      <td>0.968315</td>\n",
       "      <td>0.021185</td>\n",
       "      <td>AXP</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14851</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>0.584449</td>\n",
       "      <td>0.587924</td>\n",
       "      <td>0.589654</td>\n",
       "      <td>0.605763</td>\n",
       "      <td>0.035673</td>\n",
       "      <td>BA</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14852 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date      open      high       low     close    volume   tic  \\\n",
       "0      2009-04-01  0.018773  0.019501  0.018834  0.016610  0.313329  AAPL   \n",
       "1      2009-04-01  0.165873  0.164931  0.162829  0.121385  0.216841  AMGN   \n",
       "2      2009-04-01  0.067544  0.073365  0.066315  0.059964  0.306650   AXP   \n",
       "3      2009-04-01  0.077397  0.079819  0.077716  0.062400  0.089997    BA   \n",
       "4      2009-04-02  0.019865  0.020530  0.019902  0.017224  0.431881  AAPL   \n",
       "...           ...       ...       ...       ...       ...       ...   ...   \n",
       "14847  2023-12-28  0.586377  0.587655  0.585384  0.605043  0.049378    BA   \n",
       "14848  2023-12-29  0.979194  0.973850  0.973249  0.971834  0.022663  AAPL   \n",
       "14849  2023-12-29  0.978849  0.972427  0.990078  0.998475  0.035306  AMGN   \n",
       "14850  2023-12-29  0.950633  0.943623  0.945701  0.968315  0.021185   AXP   \n",
       "14851  2023-12-29  0.584449  0.587924  0.589654  0.605763  0.035673    BA   \n",
       "\n",
       "        day  \n",
       "0      0.50  \n",
       "1      0.50  \n",
       "2      0.50  \n",
       "3      0.50  \n",
       "4      0.75  \n",
       "...     ...  \n",
       "14847  0.75  \n",
       "14848  1.00  \n",
       "14849  1.00  \n",
       "14850  1.00  \n",
       "14851  1.00  \n",
       "\n",
       "[14852 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from finrl.meta.preprocessor.preprocessors import GroupByScaler\n",
    "\n",
    "\n",
    "# TODO - we are using the normalized indicators here, is that okay?\n",
    "\n",
    "\n",
    "portfolio_norm_df = GroupByScaler(by=\"tic\", scaler=MaxAbsScaler).fit_transform(processed)\n",
    "portfolio_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMGN</th>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AXP</th>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BA</th>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "      <td>3713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      date  open  high   low  close  volume   day\n",
       "tic                                              \n",
       "AAPL  3713  3713  3713  3713   3713    3713  3713\n",
       "AMGN  3713  3713  3713  3713   3713    3713  3713\n",
       "AXP   3713  3713  3713  3713   3713    3713  3713\n",
       "BA    3713  3713  3713  3713   3713    3713  3713"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_norm_df.groupby(\"tic\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 4\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(portfolio_norm_df.tic.unique())\n",
    "print(f\"Stock Dimension: {stock_dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMGN</th>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AXP</th>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BA</th>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      date  open  high   low  close  volume   day\n",
       "tic                                              \n",
       "AAPL  2960  2960  2960  2960   2960    2960  2960\n",
       "AMGN  2960  2960  2960  2960   2960    2960  2960\n",
       "AXP   2960  2960  2960  2960   2960    2960  2960\n",
       "BA    2960  2960  2960  2960   2960    2960  2960"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = portfolio_norm_df[(portfolio_norm_df[\"date\"] >= TRAIN_START_DATE) & (portfolio_norm_df[\"date\"] <= TRAIN_END_DATE)]\n",
    "df_2021 = portfolio_norm_df[(portfolio_norm_df[\"date\"] >= TEST_START_DATE) & (portfolio_norm_df[\"date\"] <= \"2021-12-31\")]\n",
    "df_2022 = portfolio_norm_df[(portfolio_norm_df[\"date\"] >= \"2022-01-01\") & (portfolio_norm_df[\"date\"] <= \"2022-12-31\")]\n",
    "df_2023 = portfolio_norm_df[(portfolio_norm_df[\"date\"] >= \"2023-01-01\") & (portfolio_norm_df[\"date\"] < TEST_END_DATE)]\n",
    "\n",
    " \n",
    "\n",
    "df_train.groupby(\"tic\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "Compare with ANova,\n",
    "Returns \n",
    "Drawdown period\n",
    "And sharpe ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to results/ppo\n"
     ]
    }
   ],
   "source": [
    "# from finrl.meta.env_portfolio_optimization.env_portfolio_optimization import PortfolioOptimizationEnv\n",
    "\n",
    "from finrl.agents.portfolio_optimization.models_stable import DRLStableAgent\n",
    "\n",
    "\n",
    "\n",
    "from stable_baselines3.common.logger import configure\n",
    "# Try also training a PPO agent on this same environment\n",
    "\n",
    "\n",
    "environment = PortfolioOptimizationEnv(\n",
    "        df_train,\n",
    "        initial_amount=INITIAL_CASH,\n",
    "        comission_fee_pct=COMMISSION_FEE_PERCENT,\n",
    "        # time_window=TIME_WINDOW,\n",
    "        features=[\"close\", \"high\", \"low\"], # USE THE BETA INDICATOR\n",
    "        normalize_df=None,\n",
    "        reward_scaling=1e-4,\n",
    "    )\n",
    "\n",
    "\n",
    "# TODO figuring out issues with timesteps: https://stackoverflow.com/questions/56700948/understanding-the-total-timesteps-parameter-in-stable-baselines-models\n",
    "\n",
    "agent_ppo = DRLStableAgent(env = environment)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"batch_size\": 64,\n",
    "    # \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025, # tried raising the lr which caused vanishing problem\n",
    "    # \"clip_range\": 0.2\n",
    "}\n",
    "\n",
    "# Lower clip_range makes the stocks flatter, very conservative policy\n",
    "\n",
    "# TODO try playing around with the number of epochs? n_epochs\n",
    "# TODO try playing around more with the entropy term, make sure agent does enough exploration during training\n",
    "# TODO try playing around more with the clip param here\n",
    "\n",
    "\n",
    "model_ppo = agent_ppo.get_model(\"ppo\", device, model_kwargs=PPO_PARAMS, policy_kwargs=None)\n",
    "\n",
    "# set up logger\n",
    "tmp_path = RESULTS_DIR + '/ppo'\n",
    "new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "# Set new logger\n",
    "model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of time steps in an episode:  2960\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 136  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 14   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 408820.4375\n",
      "Final accumulative portfolio value: 0.4088204375\n",
      "Maximum DrawDown: -0.8083675312225344\n",
      "Sharpe ratio: -0.21170600668902145\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -8.9e-05     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 143          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050106915 |\n",
      "|    clip_fraction        | 0.0467       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00624      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00572     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 2.96e-05     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 77668.25\n",
      "Final accumulative portfolio value: 0.07766825\n",
      "Maximum DrawDown: -0.9399716832593044\n",
      "Sharpe ratio: -0.926788838696971\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000172    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 146          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039857905 |\n",
      "|    clip_fraction        | 0.0245       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00844      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00509     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.76e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000172   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003772769 |\n",
      "|    clip_fraction        | 0.0286      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0216     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00304    |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 3.85e-05    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 265771.53125\n",
      "Final accumulative portfolio value: 0.26577153125\n",
      "Maximum DrawDown: -0.8486158709742745\n",
      "Sharpe ratio: -0.4224523144133867\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000159    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 149          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 68           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045948564 |\n",
      "|    clip_fraction        | 0.0267       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.05        |\n",
      "|    explained_variance   | 0.947        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0209      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0054      |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 1.03e-05     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 472902.8548974657\n",
      "Final accumulative portfolio value: 0.4729028548974657\n",
      "Maximum DrawDown: -0.7487799439790471\n",
      "Sharpe ratio: -0.19163428566202811\n",
      "=================================\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.96e+03   |\n",
      "|    ep_rew_mean          | -0.000138  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 144        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 84         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00369544 |\n",
      "|    clip_fraction        | 0.0225     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -6.99      |\n",
      "|    explained_variance   | 0.934      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.00738   |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0056    |\n",
      "|    std                  | 0.973      |\n",
      "|    value_loss           | 1.42e-05   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000138   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 145         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007155842 |\n",
      "|    clip_fraction        | 0.0459      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.95       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00644    |\n",
      "|    std                  | 0.97        |\n",
      "|    value_loss           | 1.2e-05     |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 178707.6875\n",
      "Final accumulative portfolio value: 0.1787076875\n",
      "Maximum DrawDown: -0.9126689786176171\n",
      "Sharpe ratio: -0.5580400650135582\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000145   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005459704 |\n",
      "|    clip_fraction        | 0.0293      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.95       |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0281     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00485    |\n",
      "|    std                  | 0.971       |\n",
      "|    value_loss           | 4.43e-06    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 232310.40625\n",
      "Final accumulative portfolio value: 0.23231040625\n",
      "Maximum DrawDown: -0.8952473253491672\n",
      "Sharpe ratio: -0.40706904109090697\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000145    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 124          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 148          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036574493 |\n",
      "|    clip_fraction        | 0.0184       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.92        |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0196      |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00349     |\n",
      "|    std                  | 0.962        |\n",
      "|    value_loss           | 4.77e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000145    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 162          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070947274 |\n",
      "|    clip_fraction        | 0.0424       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.85        |\n",
      "|    explained_variance   | 0.926        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00351     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00546     |\n",
      "|    std                  | 0.947        |\n",
      "|    value_loss           | 5.16e-06     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 263395.875\n",
      "Final accumulative portfolio value: 0.263395875\n",
      "Maximum DrawDown: -0.857680468245388\n",
      "Sharpe ratio: -0.4317664057051763\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000144   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 176         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005800141 |\n",
      "|    clip_fraction        | 0.0427      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.81       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.00822     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00553    |\n",
      "|    std                  | 0.944       |\n",
      "|    value_loss           | 3.54e-06    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 394718.96875\n",
      "Final accumulative portfolio value: 0.39471896875\n",
      "Maximum DrawDown: -0.766579799818201\n",
      "Sharpe ratio: -0.2745243570825825\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000137    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 191          |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040695453 |\n",
      "|    clip_fraction        | 0.0119       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.79        |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.000745    |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    std                  | 0.937        |\n",
      "|    value_loss           | 2.71e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000137    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 205          |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072554965 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.75        |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.000293    |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00617     |\n",
      "|    std                  | 0.931        |\n",
      "|    value_loss           | 4.47e-06     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 395775.3125\n",
      "Final accumulative portfolio value: 0.3957753125\n",
      "Maximum DrawDown: -0.7664780802001696\n",
      "Sharpe ratio: -0.232971508613363\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000132   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 220         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005185469 |\n",
      "|    clip_fraction        | 0.0287      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.7        |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00629    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00332    |\n",
      "|    std                  | 0.919       |\n",
      "|    value_loss           | 3.61e-06    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 336513.84375\n",
      "Final accumulative portfolio value: 0.33651384375\n",
      "Maximum DrawDown: -0.8108304063216506\n",
      "Sharpe ratio: -0.31031281871633737\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.00013    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 237         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004367332 |\n",
      "|    clip_fraction        | 0.0135      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.66       |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.00334     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00223    |\n",
      "|    std                  | 0.916       |\n",
      "|    value_loss           | 1.15e-06    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 408916.59375\n",
      "Final accumulative portfolio value: 0.40891659375\n",
      "Maximum DrawDown: -0.8197299483156888\n",
      "Sharpe ratio: -0.26839610192110175\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000126   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 254         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006359249 |\n",
      "|    clip_fraction        | 0.057       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.66       |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00936    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00555    |\n",
      "|    std                  | 0.917       |\n",
      "|    value_loss           | 3.79e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000126    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 268          |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062494064 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.66        |\n",
      "|    explained_variance   | 0.741        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0334      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00415     |\n",
      "|    std                  | 0.916        |\n",
      "|    value_loss           | 3.83e-06     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 445022.34375\n",
      "Final accumulative portfolio value: 0.44502234375\n",
      "Maximum DrawDown: -0.8560087682867825\n",
      "Sharpe ratio: -0.19786944712746374\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000122    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 284          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035606371 |\n",
      "|    clip_fraction        | 0.0262       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.67        |\n",
      "|    explained_variance   | 0.752        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0182       |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00283     |\n",
      "|    std                  | 0.921        |\n",
      "|    value_loss           | 8.86e-07     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 398043.375\n",
      "Final accumulative portfolio value: 0.398043375\n",
      "Maximum DrawDown: -0.804529096880591\n",
      "Sharpe ratio: -0.23003218253169536\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.00012     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 301          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064629046 |\n",
      "|    clip_fraction        | 0.0453       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.66        |\n",
      "|    explained_variance   | 0.724        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0124      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00349     |\n",
      "|    std                  | 0.913        |\n",
      "|    value_loss           | 3.69e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.00012    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 315         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004623778 |\n",
      "|    clip_fraction        | 0.0399      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.63       |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.00327     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00436    |\n",
      "|    std                  | 0.908       |\n",
      "|    value_loss           | 2.16e-06    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 850090.5625\n",
      "Final accumulative portfolio value: 0.8500905625\n",
      "Maximum DrawDown: -0.6080013094870664\n",
      "Sharpe ratio: 0.049128041084638405\n",
      "=================================\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.96e+03  |\n",
      "|    ep_rew_mean          | -0.000113 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 129       |\n",
      "|    iterations           | 21        |\n",
      "|    time_elapsed         | 330       |\n",
      "|    total_timesteps      | 43008     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0049758 |\n",
      "|    clip_fraction        | 0.0364    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.59     |\n",
      "|    explained_variance   | 0.665     |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | -0.0076   |\n",
      "|    n_updates            | 200       |\n",
      "|    policy_gradient_loss | -0.00439  |\n",
      "|    std                  | 0.902     |\n",
      "|    value_loss           | 1.3e-06   |\n",
      "---------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 379991.1875\n",
      "Final accumulative portfolio value: 0.3799911875\n",
      "Maximum DrawDown: -0.8012456099471295\n",
      "Sharpe ratio: -0.2794920884724948\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000112    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 346          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042276923 |\n",
      "|    clip_fraction        | 0.0238       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.58        |\n",
      "|    explained_variance   | 0.557        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00826     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00266     |\n",
      "|    std                  | 0.904        |\n",
      "|    value_loss           | 3.32e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000112   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 360         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006824593 |\n",
      "|    clip_fraction        | 0.0649      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.57       |\n",
      "|    explained_variance   | 0.509       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0335     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00694    |\n",
      "|    std                  | 0.898       |\n",
      "|    value_loss           | 4.46e-06    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 256929.078125\n",
      "Final accumulative portfolio value: 0.256929078125\n",
      "Maximum DrawDown: -0.8602758272758305\n",
      "Sharpe ratio: -0.4127385209089822\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000113    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 376          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073242933 |\n",
      "|    clip_fraction        | 0.0748       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.55        |\n",
      "|    explained_variance   | 0.58         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00408     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00688     |\n",
      "|    std                  | 0.897        |\n",
      "|    value_loss           | 7.46e-06     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 349300.8125\n",
      "Final accumulative portfolio value: 0.3493008125\n",
      "Maximum DrawDown: -0.8333432869090829\n",
      "Sharpe ratio: -0.30448504362112877\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000113   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 391         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006121346 |\n",
      "|    clip_fraction        | 0.0253      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.57       |\n",
      "|    explained_variance   | 0.511       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00227    |\n",
      "|    std                  | 0.904       |\n",
      "|    value_loss           | 2.25e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000113    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 131          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 404          |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059781307 |\n",
      "|    clip_fraction        | 0.051        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.56        |\n",
      "|    explained_variance   | 0.279        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00222     |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.0059      |\n",
      "|    std                  | 0.896        |\n",
      "|    value_loss           | 2.17e-06     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 657374.5\n",
      "Final accumulative portfolio value: 0.6573745\n",
      "Maximum DrawDown: -0.7781210229482672\n",
      "Sharpe ratio: -0.018226490569423378\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000109   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 420         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007133199 |\n",
      "|    clip_fraction        | 0.0498      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.54       |\n",
      "|    explained_variance   | 0.0655      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0332     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00459    |\n",
      "|    std                  | 0.894       |\n",
      "|    value_loss           | 3.27e-06    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 453776.25\n",
      "Final accumulative portfolio value: 0.45377625\n",
      "Maximum DrawDown: -0.7631821965617167\n",
      "Sharpe ratio: -0.16665091774719698\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000107   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 435         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007482495 |\n",
      "|    clip_fraction        | 0.0688      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.54       |\n",
      "|    explained_variance   | -0.434      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00262    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00642    |\n",
      "|    std                  | 0.895       |\n",
      "|    value_loss           | 5.76e-07    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 453131.5625\n",
      "Final accumulative portfolio value: 0.4531315625\n",
      "Maximum DrawDown: -0.8311333976111593\n",
      "Sharpe ratio: -0.17134577344701113\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000106    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 131          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 450          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068636783 |\n",
      "|    clip_fraction        | 0.0684       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.53        |\n",
      "|    explained_variance   | -0.941       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00553      |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00537     |\n",
      "|    std                  | 0.894        |\n",
      "|    value_loss           | 4.26e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000106    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 132          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 463          |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052670673 |\n",
      "|    clip_fraction        | 0.0487       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.54        |\n",
      "|    explained_variance   | -1.32        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00976     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00562     |\n",
      "|    std                  | 0.898        |\n",
      "|    value_loss           | 1.17e-06     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 327692.46875\n",
      "Final accumulative portfolio value: 0.32769246875\n",
      "Maximum DrawDown: -0.8587711802215863\n",
      "Sharpe ratio: -0.3009474790325136\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000106    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 132          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 478          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064454274 |\n",
      "|    clip_fraction        | 0.0609       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.55        |\n",
      "|    explained_variance   | -0.366       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0103      |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00632     |\n",
      "|    std                  | 0.898        |\n",
      "|    value_loss           | 3.41e-07     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 394952.9375\n",
      "Final accumulative portfolio value: 0.3949529375\n",
      "Maximum DrawDown: -0.8039707647556931\n",
      "Sharpe ratio: -0.22924102421570933\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000106   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 493         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005622694 |\n",
      "|    clip_fraction        | 0.0573      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.56       |\n",
      "|    explained_variance   | -0.793      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00453    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00443    |\n",
      "|    std                  | 0.899       |\n",
      "|    value_loss           | 2.6e-06     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000106   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 506         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006200097 |\n",
      "|    clip_fraction        | 0.0436      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.58       |\n",
      "|    explained_variance   | -1.51       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.0154      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00407    |\n",
      "|    std                  | 0.905       |\n",
      "|    value_loss           | 1.51e-05    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 712950.3125\n",
      "Final accumulative portfolio value: 0.7129503125\n",
      "Maximum DrawDown: -0.8246481484315716\n",
      "Sharpe ratio: 0.013853916771306214\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000102    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 133          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 521          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036436939 |\n",
      "|    clip_fraction        | 0.018        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.61        |\n",
      "|    explained_variance   | -0.0388      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00654      |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00159     |\n",
      "|    std                  | 0.91         |\n",
      "|    value_loss           | 3.78e-07     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 193820.65625\n",
      "Final accumulative portfolio value: 0.19382065625\n",
      "Maximum DrawDown: -0.8683701714663599\n",
      "Sharpe ratio: -0.5115404170440574\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000105   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 536         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004097686 |\n",
      "|    clip_fraction        | 0.0233      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.62       |\n",
      "|    explained_variance   | 0.0608      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.00507     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00286    |\n",
      "|    std                  | 0.91        |\n",
      "|    value_loss           | 5.77e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000105   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 548         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006594265 |\n",
      "|    clip_fraction        | 0.0445      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.63       |\n",
      "|    explained_variance   | 0.0837      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0217     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00518    |\n",
      "|    std                  | 0.912       |\n",
      "|    value_loss           | 6.25e-07    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 180102.015625\n",
      "Final accumulative portfolio value: 0.180102015625\n",
      "Maximum DrawDown: -0.8796563495212099\n",
      "Sharpe ratio: -0.5271660021594948\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000108    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 134          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 562          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066230395 |\n",
      "|    clip_fraction        | 0.0489       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.59        |\n",
      "|    explained_variance   | 0.0914       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0277      |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00512     |\n",
      "|    std                  | 0.9          |\n",
      "|    value_loss           | 3.22e-07     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 368761.0625\n",
      "Final accumulative portfolio value: 0.3687610625\n",
      "Maximum DrawDown: -0.8445115970742609\n",
      "Sharpe ratio: -0.2485747066112179\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000107    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 134          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 577          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063029565 |\n",
      "|    clip_fraction        | 0.052        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.56        |\n",
      "|    explained_variance   | -0.432       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0253      |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00463     |\n",
      "|    std                  | 0.899        |\n",
      "|    value_loss           | 5.32e-07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000107   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 592         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005109215 |\n",
      "|    clip_fraction        | 0.0332      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.56       |\n",
      "|    explained_variance   | -0.343      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0182     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00264    |\n",
      "|    std                  | 0.9         |\n",
      "|    value_loss           | 1.29e-06    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 236900.125\n",
      "Final accumulative portfolio value: 0.236900125\n",
      "Maximum DrawDown: -0.8635877623587347\n",
      "Sharpe ratio: -0.43230439680324356\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000109   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 608         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007429989 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.56       |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00748    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00507    |\n",
      "|    std                  | 0.9         |\n",
      "|    value_loss           | 4.04e-06    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 523264.90625\n",
      "Final accumulative portfolio value: 0.52326490625\n",
      "Maximum DrawDown: -0.7889323660224923\n",
      "Sharpe ratio: -0.1384089604594755\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000107   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 625         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006533456 |\n",
      "|    clip_fraction        | 0.0909      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.55       |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0273     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00828    |\n",
      "|    std                  | 0.897       |\n",
      "|    value_loss           | 2.54e-07    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 219097.71875\n",
      "Final accumulative portfolio value: 0.21909771875\n",
      "Maximum DrawDown: -0.9091372596807141\n",
      "Sharpe ratio: -0.44890961123814416\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000109    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 134          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 641          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052610696 |\n",
      "|    clip_fraction        | 0.0332       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.55        |\n",
      "|    explained_variance   | 0.688        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0101      |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00307     |\n",
      "|    std                  | 0.897        |\n",
      "|    value_loss           | 5.62e-07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000109    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 134          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 653          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071541886 |\n",
      "|    clip_fraction        | 0.0732       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.57        |\n",
      "|    explained_variance   | 0.497        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0172       |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00439     |\n",
      "|    std                  | 0.903        |\n",
      "|    value_loss           | 8.75e-07     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 208040.9375\n",
      "Final accumulative portfolio value: 0.2080409375\n",
      "Maximum DrawDown: -0.915334617863351\n",
      "Sharpe ratio: -0.43344177139400003\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.00011     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 134          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 667          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069906795 |\n",
      "|    clip_fraction        | 0.0715       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.55        |\n",
      "|    explained_variance   | 0.676        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00627     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00712     |\n",
      "|    std                  | 0.894        |\n",
      "|    value_loss           | 9.18e-07     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 384864.5625\n",
      "Final accumulative portfolio value: 0.3848645625\n",
      "Maximum DrawDown: -0.822832725886605\n",
      "Sharpe ratio: -0.25683492366730915\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.00011    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 681         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008245644 |\n",
      "|    clip_fraction        | 0.0647      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.53       |\n",
      "|    explained_variance   | 0.591       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.0102      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00509    |\n",
      "|    std                  | 0.895       |\n",
      "|    value_loss           | 8.15e-07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.00011    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 695         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007955322 |\n",
      "|    clip_fraction        | 0.0721      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.52       |\n",
      "|    explained_variance   | 0.749       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.00235     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00717    |\n",
      "|    std                  | 0.89        |\n",
      "|    value_loss           | 5.98e-06    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 435215.25\n",
      "Final accumulative portfolio value: 0.43521525\n",
      "Maximum DrawDown: -0.8093200783520083\n",
      "Sharpe ratio: -0.18330381644095914\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000109   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 710         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006803118 |\n",
      "|    clip_fraction        | 0.0477      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.52       |\n",
      "|    explained_variance   | 0.502       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.0142      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00432    |\n",
      "|    std                  | 0.894       |\n",
      "|    value_loss           | 3.48e-07    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 212130.83116857483\n",
      "Final accumulative portfolio value: 0.21213083116857484\n",
      "Maximum DrawDown: -0.8751917967219094\n",
      "Sharpe ratio: -0.4803858346883751\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.00011     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 135          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 727          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072501753 |\n",
      "|    clip_fraction        | 0.0544       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.52        |\n",
      "|    explained_variance   | -0.156       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0012      |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00533     |\n",
      "|    std                  | 0.89         |\n",
      "|    value_loss           | 3.72e-07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.00011     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 135          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 741          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063780984 |\n",
      "|    clip_fraction        | 0.0398       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.51        |\n",
      "|    explained_variance   | -0.793       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00125      |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00416     |\n",
      "|    std                  | 0.891        |\n",
      "|    value_loss           | 7.65e-07     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 394103.0\n",
      "Final accumulative portfolio value: 0.394103\n",
      "Maximum DrawDown: -0.7254643869715531\n",
      "Sharpe ratio: -0.26653294802590644\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.00011     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 135          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 758          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072803893 |\n",
      "|    clip_fraction        | 0.0637       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.5         |\n",
      "|    explained_variance   | -0.871       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00139     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00417     |\n",
      "|    std                  | 0.886        |\n",
      "|    value_loss           | 5.74e-07     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 227923.05817259446\n",
      "Final accumulative portfolio value: 0.22792305817259448\n",
      "Maximum DrawDown: -0.8451591465140287\n",
      "Sharpe ratio: -0.44670104237224567\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000111   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 774         |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006450204 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | -0.509      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.0145      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00435    |\n",
      "|    std                  | 0.888       |\n",
      "|    value_loss           | 2.55e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000111    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 135          |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 788          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063243713 |\n",
      "|    clip_fraction        | 0.056        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.51        |\n",
      "|    explained_variance   | -0.317       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0457       |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.00257     |\n",
      "|    std                  | 0.892        |\n",
      "|    value_loss           | 1.41e-06     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 170341.375\n",
      "Final accumulative portfolio value: 0.170341375\n",
      "Maximum DrawDown: -0.8905012985659407\n",
      "Sharpe ratio: -0.5981141148169019\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.000113    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 134          |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 804          |\n",
      "|    total_timesteps      | 108544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066911886 |\n",
      "|    clip_fraction        | 0.0548       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.53        |\n",
      "|    explained_variance   | -0.208       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00329     |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.00577     |\n",
      "|    std                  | 0.896        |\n",
      "|    value_loss           | 5.14e-06     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 701738.0625\n",
      "Final accumulative portfolio value: 0.7017380625\n",
      "Maximum DrawDown: -0.6656510558543363\n",
      "Sharpe ratio: -0.022638672190328616\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000111   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 820         |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006424481 |\n",
      "|    clip_fraction        | 0.0679      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | 0.321       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00937    |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00593    |\n",
      "|    std                  | 0.885       |\n",
      "|    value_loss           | 7.07e-08    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 431358.65625\n",
      "Final accumulative portfolio value: 0.43135865625\n",
      "Maximum DrawDown: -0.8245707024731501\n",
      "Sharpe ratio: -0.23143857716630503\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.00011     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 134          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 836          |\n",
      "|    total_timesteps      | 112640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081108995 |\n",
      "|    clip_fraction        | 0.0683       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.49        |\n",
      "|    explained_variance   | 0.578        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0207      |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00605     |\n",
      "|    std                  | 0.889        |\n",
      "|    value_loss           | 3.26e-07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.00011     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 134          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 850          |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070521063 |\n",
      "|    clip_fraction        | 0.07         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.51        |\n",
      "|    explained_variance   | -0.831       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0359      |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.00457     |\n",
      "|    std                  | 0.891        |\n",
      "|    value_loss           | 1.61e-06     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 212844.203125\n",
      "Final accumulative portfolio value: 0.212844203125\n",
      "Maximum DrawDown: -0.8884865410551601\n",
      "Sharpe ratio: -0.41918554472133085\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.96e+03    |\n",
      "|    ep_rew_mean          | -0.000111   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 866         |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006943336 |\n",
      "|    clip_fraction        | 0.0653      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | 0.288       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00965    |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.00483    |\n",
      "|    std                  | 0.892       |\n",
      "|    value_loss           | 5.99e-08    |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 470397.1875\n",
      "Final accumulative portfolio value: 0.4703971875\n",
      "Maximum DrawDown: -0.7843586292583283\n",
      "Sharpe ratio: -0.19534966248901697\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.96e+03     |\n",
      "|    ep_rew_mean          | -0.00011     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 134          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 882          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069375043 |\n",
      "|    clip_fraction        | 0.0567       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.52        |\n",
      "|    explained_variance   | -0.896       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0106       |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00609     |\n",
      "|    std                  | 0.892        |\n",
      "|    value_loss           | 2.81e-07     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Once optimal policy is learned it shouldnt be stochastic, giventhe state\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m  \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model_ppo \u001b[38;5;241m=\u001b[39m \u001b[43mDRLStableAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ppo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfinrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TRAINED_MODEL_DIR\n\u001b[0;32m      8\u001b[0m model_ppo\u001b[38;5;241m.\u001b[39msave(TRAINED_MODEL_DIR \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/agent_opt_ppo_update\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\finrl\\agents\\portfolio_optimization\\models_stable.py:79\u001b[0m, in \u001b[0;36mDRLStableAgent.train_model\u001b[1;34m(model, env, episodes)\u001b[0m\n\u001b[0;32m     75\u001b[0m max_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(env\u001b[38;5;241m.\u001b[39m_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax number of time steps in an episode: \u001b[39m\u001b[38;5;124m\"\u001b[39m, max_steps)\n\u001b[1;32m---> 79\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m  \u001b[39;49;00m\n\u001b[0;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:313\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\ppo\\ppo.py:279\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 279\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[0;32m    281\u001b[0m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Once optimal policy is learned it shouldnt be stochastic, giventhe state\n",
    "\n",
    "  \n",
    "model_ppo = DRLStableAgent.train_model(model_ppo, env=environment, episodes=50)\n",
    "\n",
    "from finrl.config import TRAINED_MODEL_DIR\n",
    "\n",
    "model_ppo.save(TRAINED_MODEL_DIR + \"/agent_opt_ppo_update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_2021.index.unique()) - 1)\n",
    "print(df_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_results = {\n",
    "    \"date\": environment._terminal_date_memory,\n",
    "    \"training\": environment._terminal_asset_memory[\"final\"],\n",
    "    \"2021\": {},\n",
    "    \"2022\": {},\n",
    "    \"2023\": {}\n",
    "}\n",
    "\n",
    "\n",
    "environment_2021 = PortfolioOptimizationEnv(\n",
    "    df_2021,\n",
    "    initial_amount=INITIAL_CASH,\n",
    "    comission_fee_pct=COMMISSION_FEE_PERCENT,\n",
    "    # time_window=TIME_WINDOW,\n",
    "    features=[\"close\", \"high\", \"low\"],\n",
    "    normalize_df=None,\n",
    "        reward_scaling=1e-4,\n",
    ")\n",
    "\n",
    "\n",
    "environment_2022 = PortfolioOptimizationEnv(\n",
    "    df_2022,\n",
    "    initial_amount=INITIAL_CASH,\n",
    "    comission_fee_pct=COMMISSION_FEE_PERCENT,\n",
    "    # time_window=TIME_WINDOW,\n",
    "    features=[\"close\", \"high\", \"low\"],\n",
    "    normalize_df=None,\n",
    "        reward_scaling=1e-4,\n",
    ")\n",
    "\n",
    "environment_2023 = PortfolioOptimizationEnv(\n",
    "    df_2023,\n",
    "    initial_amount=INITIAL_CASH,\n",
    "    comission_fee_pct=COMMISSION_FEE_PERCENT,\n",
    "    # time_window=TIME_WINDOW,\n",
    "    features=[\"close\", \"high\", \"low\"],\n",
    "    normalize_df=None,\n",
    "        reward_scaling=1e-4,\n",
    ")\n",
    "\n",
    "\n",
    "print(model_ppo._num_timesteps_at_start)\n",
    "\n",
    "# 2021\n",
    "values, dates = DRLStableAgent.DRL_prediction(model_ppo, environment_2021)\n",
    "PPO_results[\"2021\"][\"value\"] = environment_2021._terminal_asset_memory[\"final\"]\n",
    "PPO_results[\"2021\"][\"date\"] = environment_2021._terminal_date_memory\n",
    "\n",
    "\n",
    "# 2022\n",
    "values, dates = DRLStableAgent.DRL_prediction(model_ppo, environment_2022)\n",
    "PPO_results[\"2022\"][\"value\"] = environment_2022._terminal_asset_memory[\"final\"]\n",
    "PPO_results[\"2022\"][\"date\"] = environment_2022._terminal_date_memory\n",
    "\n",
    "# # 2023\n",
    "# DRLStableAgent.DRL_prediction(model_ppo, environment_2023)\n",
    "# PPO_results[\"2023\"][\"value\"] = environment_2023._asset_memory[\"final\"]\n",
    "# PPO_results[\"2023\"][\"date\"] = environment_2023._date_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_2021['date'].unique()) - 1)\n",
    "\n",
    "print(len(PPO_results['date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "plt.plot(PPO_results[\"date\"], PPO_results[\"training\"], label=\"PPO\")\n",
    "\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Portfolio Value\")\n",
    "plt.title(\"Performance in training period\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UBAH_results = {\n",
    "    \"train\": {},\n",
    "    \"2021\": {},\n",
    "    \"2022\": {},\n",
    "    \"2023\": {},\n",
    "}\n",
    "\n",
    "PORTFOLIO_SIZE = len(DOW_30_TICKER)\n",
    "\n",
    "\n",
    "\n",
    "# This is the CRP strategy NOT Buy and hold \n",
    "# train period\n",
    "terminated = False\n",
    "environment.reset()\n",
    "while not terminated:\n",
    "    action = [0] + [1/PORTFOLIO_SIZE] * PORTFOLIO_SIZE\n",
    "    _, _, terminated, _ = environment.step(action)\n",
    "UBAH_results[\"train\"][\"value\"] = environment._terminal_asset_memory[\"final\"]\n",
    "UBAH_results[\"train\"][\"date\"] = environment._terminal_date_memory\n",
    "\n",
    "# 2021\n",
    "terminated = False\n",
    "environment_2021.reset()\n",
    "while not terminated:\n",
    "    action = [0] + [1/PORTFOLIO_SIZE] * PORTFOLIO_SIZE\n",
    "    _, _, terminated, _ = environment_2021.step(action)\n",
    "UBAH_results[\"2021\"][\"value\"] = environment_2021._terminal_asset_memory[\"final\"]\n",
    "UBAH_results[\"2021\"][\"date\"] = environment_2021._terminal_date_memory\n",
    "\n",
    "# 2022\n",
    "terminated = False\n",
    "environment_2022.reset()\n",
    "while not terminated:\n",
    "    action = [0] + [1/PORTFOLIO_SIZE] * PORTFOLIO_SIZE\n",
    "    _, _, terminated, _ = environment_2022.step(action)\n",
    "UBAH_results[\"2022\"][\"value\"] = environment_2022._terminal_asset_memory[\"final\"]\n",
    "UBAH_results[\"2022\"][\"date\"] = environment_2022._terminal_date_memory\n",
    "\n",
    "# # 2023\n",
    "# terminated = False\n",
    "# environment_2023.reset()\n",
    "# while not terminated:\n",
    "#     action = [0] + [1/PORTFOLIO_SIZE] * PORTFOLIO_SIZE\n",
    "#     _, _, terminated, _ = environment_2023.step(action)\n",
    "# UBAH_results[\"2023\"][\"value\"] = environment_2023._terminal_asset_memory[\"final\"]\n",
    "# UBAH_results[\"2023\"][\"date\"] = environment_2023._terminal_date_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "plt.plot(UBAH_results[\"train\"][\"date\"], UBAH_results[\"train\"][\"value\"], label=\"Buy and Hold\")\n",
    "plt.plot(PPO_results[\"date\"], PPO_results[\"training\"], label=\"PPO\")\n",
    "\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Portfolio Value\")\n",
    "plt.title(\"Performance in training period\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(UBAH_results[\"2021\"][\"date\"], UBAH_results[\"2021\"][\"value\"], label=\"Buy and Hold\")\n",
    "plt.plot(PPO_results[\"2021\"][\"date\"], PPO_results[\"2021\"][\"value\"], label=\"PPO\")\n",
    "\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Portfolio Value\")\n",
    "plt.title(\"Performance in 2021\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(UBAH_results[\"2022\"][\"date\"], UBAH_results[\"2022\"][\"value\"], label=\"Buy and Hold\")\n",
    "plt.plot(PPO_results[\"2022\"][\"date\"], PPO_results[\"2022\"][\"value\"], label=\"PPO\")\n",
    "\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Portfolio Value\")\n",
    "plt.title(\"Performance in 2022\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(UBAH_results[\"2023\"][\"date\"], UBAH_results[\"2023\"][\"value\"], label=\"Buy and Hold\")\n",
    "# plt.plot(PPO_results[\"2023\"][\"date\"], PPO_results[\"2023\"][\"value\"], label=\"PPO\")\n",
    "\n",
    "# plt.xlabel(\"Days\")\n",
    "# plt.ylabel(\"Portfolio Value\")\n",
    "# plt.title(\"Performance in 2023\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.make_archive('train', 'zip', '../content')\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download(\"train.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
