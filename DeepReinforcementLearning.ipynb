{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a deep reinforcement\n",
    "learning agent and obtain an ensemble trading strategy\n",
    "using three actor-critic based algorithms: \n",
    "Proximal Policy\n",
    "Optimization (PPO), \n",
    "Advantage Actor Critic (A2C), \n",
    "Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "\n",
    "\n",
    "The proposed deep ensemble strategy is shown to outperform\n",
    "the three individual algorithms and two baselines in terms of\n",
    "the risk-adjusted return measured by the Sharpe ratio.\n",
    "\n",
    "\n",
    "Fundamentals\n",
    "data (earnings report) and alternative data (market news,\n",
    "academic graph data, credit card transactions, and GPS\n",
    "traffic, etc.) are combined with machine learning algorithms\n",
    "to extract new investment alphas or predict a company’s\n",
    "future performance\n",
    "\n",
    "Thus, a predictive\n",
    "alpha signal is generated to perform stock selection.\n",
    "\n",
    "\n",
    "In this paper, we propose a novel ensemble strategy\n",
    "that combines three deep reinforcement learning algorithms\n",
    "and finds the optimal trading strategy in a complex and\n",
    "dynamic stock market.\n",
    "\n",
    "\n",
    "First, we build an environment and define\n",
    "action space, state space, and reward function.\n",
    "\n",
    "Second, we\n",
    "train the three algorithms that take actions in the environment.\n",
    "\n",
    "Third, we ensemble the three agents together using\n",
    "the Sharpe ratio that measures the risk-adjusted return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# critic-only learning approach\n",
    "\n",
    "The critic-only learning approach, which is the most\n",
    "common, solves a discrete action space problem using, for\n",
    "example, Deep Q-learning (DQN) and its improvements,\n",
    "and trains an agent on a single stock or asset\n",
    "\n",
    "\n",
    "idea of the critic-only approach is to use a Qvalue\n",
    "function to learn the optimal action-selection policy\n",
    "that maximizes the expected future reward given the current\n",
    "state.\n",
    "\n",
    "It focuses solely on evaluating the value function without explicitly learning a policy\n",
    "\n",
    "This approach is often used in value-based methods like Q-learning and Deep Q-Networks\n",
    "\n",
    "The major limitation of\n",
    "the critic-only approach is that it only works with discrete\n",
    "and finite state and action spaces, which is not practical for\n",
    "a large portfolio of stocks, since the prices are of course\n",
    "continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# actor-only approach\n",
    "\n",
    "The idea here is that the agent directly learns the optimal\n",
    "policy itself.\n",
    "\n",
    "Instead of having a neural network to learn the\n",
    "Q-value, the neural network learns the policy.\n",
    "\n",
    "The policy is\n",
    "a probability distribution that is essentially a strategy for a\n",
    "given state, namely the likelihood to take an allowed action.\n",
    "\n",
    "\n",
    "An actor-only approach in Reinforcement Learning (RL) focuses on learning a policy directly without maintaining a value function. This is typically done using policy gradient methods, where the policy is optimized based on feedback from the environment.\n",
    "\n",
    "Key Features\n",
    "Policy Gradient: Directly optimizes the policy by adjusting its parameters in the direction of the gradient of expected rewards.\n",
    "\n",
    "Exploration: Ensures sufficient exploration of the action space to improve the policy.\n",
    "\n",
    "Convergence: Can converge to local optima, which may be more efficient in certain environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# actor-critic approach\n",
    "\n",
    "\n",
    "Reinforcement Learning (RL) combines elements of both policy-based (actor) and value-based (critic) methods. The actor decides the actions to take based on a policy, while the critic evaluates those actions by estimating the value function.\n",
    "\n",
    "Over time, the actor learns to take better\n",
    "actions and the critic gets better at evaluating those actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP Model for Stock Trading\n",
    "\n",
    "We model stock trading as a Markov Decision Process\n",
    "(MDP)\n",
    "\n",
    "State s = [p;h; b]: a vector that includes stock prices\n",
    "p 2 RD+\n",
    ", the stock shares h 2 ZD+\n",
    ", and the remaining\n",
    "balance b 2 R+, where D denotes the number of\n",
    "stocks and Z+ denotes non-negative integers.\n",
    "\n",
    "\n",
    "\n",
    "Action a: a vector of actions over D stocks. The\n",
    "allowed actions on each stock include selling, buying,\n",
    "or holding, which result in decreasing, increasing, and\n",
    "no change of the stock shares h, respectively.\n",
    "\n",
    "\n",
    "Reward r(s; a; s0): the direct reward of taking action\n",
    "a at state s and arriving at the new state s0.\n",
    "\n",
    "\n",
    "Policy \u0019(s): the trading strategy at state s, which is\n",
    "the probability distribution of actions at state s.\n",
    "\n",
    "\n",
    "Q-value Q\u0019(s; a): the expected reward of taking action\n",
    "a at state s following policy \u0019.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "At each state, one of three possible actions is\n",
    "taken on stock d\n",
    "\n",
    "Selling k[d] 2 [1;h[d]] shares results in ht+1[d] =\n",
    "ht[d] 􀀀 k[d], where k[d] 2 Z+ and d = 1; :::;D.\n",
    "\n",
    "Holding, ht+1[d] = ht[d].\n",
    "\n",
    "Buying k[d] shares results in ht+1[d] = ht[d]+k[d].\n",
    "\n",
    "\n",
    "At time t an action is taken and the stock prices update\n",
    "at t+1, accordingly the portfolio values may change from\n",
    "”portfolio value 0” to ”portfolio value 1”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating Stock Trading Constraints\n",
    "\n",
    "Assumptions:\n",
    "Market liquidity: the orders can be rapidly executed at\n",
    "the close price.\n",
    "We assume that stock market will not\n",
    "be affected by our reinforcement trading agent.\n",
    "\n",
    "Nonnegative balance b >= 0: the allowed actions should\n",
    "not result in a negative balance.\n",
    "\n",
    "Transaction cost: transaction costs are incurred for\n",
    "each trade.\n",
    "\n",
    "Risk-aversion for market crash: there are sudden\n",
    "events that may cause stock market crash, such as\n",
    "wars, collapse of stock market bubbles, sovereign debt\n",
    "default, and financial crisis.\n",
    "\n",
    "we employ the financial turbulence index turbulence to measure extreme asset price movements\n",
    "\n",
    "When turbulencet is higher than a\n",
    "threshold, which indicates extreme market conditions,\n",
    "we simply halt buying and the trading agent sells all\n",
    "shares.\n",
    "\n",
    "We resume trading once the turbulence index\n",
    "returns under the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return Maximization as Trading Goal\n",
    "\n",
    "We define our reward function as the change of the\n",
    "portfolio value when action a is taken at state s and arriving\n",
    "at new state s0.\n",
    "\n",
    "Goal is to design a trading strategy that\n",
    "maximizes the change of the portfolio value\n",
    "\n",
    "\n",
    "The optimal strategy is defined by the BEllman equation on fomrula 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Space: For a single stock, the action space\n",
    "is defined as f􀀀k; :::;􀀀1; 0; 1; :::; kg, where k and 􀀀k\n",
    "\n",
    "\n",
    "The action space is then normalized to [􀀀1; 1], since the\n",
    "RL algorithms A2C and PPO define the policy directly on\n",
    "a Gaussian distribution, which needs to be normalized and\n",
    "symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantage Actor Critic (A2C)\n",
    "\n",
    "is a typical actor-critic algorithm and we use\n",
    "it a component in the ensemble strategy\n",
    "\n",
    "A2C utilizes an\n",
    "advantage function to reduce the variance of the policy\n",
    "gradient.\n",
    "\n",
    "the\n",
    "evaluation of an action not only depends on how good the\n",
    "action is, but also considers how much better it can be. So\n",
    "that it reduces the high variance of the policy network and\n",
    "makes the model more robust.\n",
    "\n",
    "\n",
    "A2C is a great model for stock trading because of its\n",
    "stability.\n",
    "\n",
    "\n",
    "Objective function on formula 12\n",
    "\n",
    "Advantage function formula 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient\n",
    "\n",
    "DDPG [18] is used to encourage maximum investment\n",
    "return.\n",
    "\n",
    "\n",
    "DDPG combines the frameworks of both Q-learning\n",
    "[38] and policy gradient, uses neural networks as\n",
    "function approximators.\n",
    "\n",
    "DDPG learns directly from\n",
    "the observations through policy gradient. It is proposed\n",
    "to deterministically map states to actions to better fit the\n",
    "continuous action space environment.\n",
    "\n",
    "\n",
    "\n",
    "The Policy Gradient method is a reinforcement learning approach that directly optimizes the policy by adjusting its parameters based on the gradients of expected rewards. It’s particularly useful for handling continuous action spaces and complex policies.\n",
    "\n",
    "Key Features\n",
    "Direct Optimization: Adjusts the policy parameters to maximize the expected return.\n",
    "\n",
    "Stochastic Policies: Often uses stochastic policies, which are particularly useful in exploration.\n",
    "\n",
    "Gradient Ascent: Uses gradient ascent to update the policy parameters in the direction of higher expected reward.\n",
    "\n",
    "\n",
    "\n",
    "DDPG is effective at handling continuous action space, and\n",
    "so it is appropriate for stock trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Environment setup (simplified example)\n",
    "num_actions = 2\n",
    "num_states = 4\n",
    "\n",
    "# Policy Model - Neural network that outputs a probability distribution over actions.\n",
    "policy_model = tf.keras.Sequential([\n",
    "    layers.Dense(24, activation='relu', input_shape=(num_states,)),\n",
    "    layers.Dense(24, activation='relu'),\n",
    "    layers.Dense(num_actions, activation='softmax')\n",
    "])\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Training function - Updates the policy parameters using the policy gradient.\n",
    "def train_policy_gradient(state, action, reward, next_state, done):\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_probs = policy_model(state, training=True)\n",
    "        chosen_action_prob = action_probs[0, action]\n",
    "        log_prob = tf.math.log(chosen_action_prob)\n",
    "        loss = -log_prob * reward\n",
    "\n",
    "    grads = tape.gradient(loss, policy_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, policy_model.trainable_variables))\n",
    "\n",
    "# Simplified training loop - Simulates episodes and updates the policy based on observed rewards.\n",
    "for episode in range(1000):\n",
    "    state = np.random.rand(num_states)  # Random initial state\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "\n",
    "    while not done:\n",
    "        action_probs = policy_model(state[np.newaxis, :])\n",
    "        action = np.random.choice(num_actions, p=action_probs.numpy().flatten())\n",
    "        next_state = np.random.rand(num_states)  # Random next state\n",
    "        reward = np.random.randn()  # Random reward\n",
    "        done = np.random.choice([0, 1])  # Random done flag\n",
    "\n",
    "        episode_rewards.append(reward)\n",
    "        train_policy_gradient(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization\n",
    "\n",
    "PPO [14] is introduced to control the policy\n",
    "gradient update and ensure that the new policy will not be\n",
    "too different from the previous one. PPO tries to simplify\n",
    "the objective of Trust Region Policy Optimization (TRPO)\n",
    "by introducing a clipping term to the objective function\n",
    "\n",
    "The\n",
    "function clip(rt(\u0012); 1 􀀀 \u000f; 1 + \u000f) clips the ratio rt(\u0012) to be\n",
    "within [1 􀀀 \u000f; 1 + \u000f]. The objective function of PPO takes\n",
    "the minimum of the clipped and normal objective.\n",
    "\n",
    "PPO\n",
    "discourages large policy change move outside of the clipped\n",
    "interval. Therefore, PPO improves the stability of the policy\n",
    "networks training by restricting the policy update at each\n",
    "training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble strategy\n",
    "\n",
    "So we use an ensemble strategy to automatically select the\n",
    "best performing agent among PPO, A2C, and DDPG to\n",
    "trade based on the Sharpe ratio.\n",
    "\n",
    "\n",
    "Step 1. We use a growing window of n months to retrain\n",
    "our three agents concurrently. In this paper we retrain our\n",
    "three agents at every three months.\n",
    "\n",
    "Step 2. We validate all three agents by using a 3-month\n",
    "validation rolling window after training window to pick the\n",
    "best performing agent with the highest Sharpe ratio\n",
    "\n",
    "We also adjust risk-aversion by using turbulence index in our validation stage.\n",
    "\n",
    "\n",
    "\n",
    "#TODO =========================\n",
    "\n",
    "Step 3. After the best agent is picked, we use it to predict\n",
    "and trade for the next quarter.\n",
    "\n",
    "\n",
    "The\n",
    "higher an agent’s Sharpe ratio, the better its returns have\n",
    "been relative to the amount of investment risk it has taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Data Preprocessing\n",
    "\n",
    "Our dataset consists\n",
    "of two periods: in-sample period and out-of-sample period.\n",
    "In-sample period contains data for training and validation\n",
    "stages. Out-of-sample period contains data for trading stage.\n",
    "\n",
    "\n",
    "Then, a validation stage is then\n",
    "carried out for validating the 3 agents by Sharpe ratio, and\n",
    "adjusting key parameters, such as learning rate, number of\n",
    "episodes, etc.\n",
    "\n",
    "\n",
    "so we use A2C to trade for the next quarter \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Comparisons\n",
    "\n",
    "1. Cumulative return: is calculated by subtracting the\n",
    "portfolio’s final value from its initial value, and then\n",
    "dividing by the initial value.\n",
    "2. Annualized return: is the geometric average amount\n",
    "of money earned by the agent each year over the time\n",
    "period.\n",
    "3. Annualized volatility: is the annualized standard deviation\n",
    "of portfolio return.\n",
    "4. Sharpe ratio: is calculated by subtracting the annualized\n",
    "risk free rate from the annualized return, and the\n",
    "dividing by the annualized volatility.\n",
    "5. Max drawdown: is the maximum percentage loss during\n",
    "the trading period.\n",
    "\n",
    "\n",
    "Analysis of Agent Performance: From both Table 2\n",
    "and Figure 5, we can observe that the A2C agent is more\n",
    "adaptive to risk. bearish market.\n",
    "\n",
    "\n",
    "PPO agent\n",
    "is good at following trend and acts well in generating\n",
    "more returns, it has the highest annual return\n",
    "\n",
    "So PPO\n",
    "is preferred when facing a bullish market. DDPG performs\n",
    "similar but not as good as PPO, it can be used as a\n",
    "complementary strategy to PPO in a bullish market.\n",
    "\n",
    "\n",
    "\n",
    "By incorporating the turbulence\n",
    "index, the agents are able to cut losses and successfully\n",
    "survive the stock market crash in March 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More reading online\n",
    "\n",
    "\n",
    "Q-learning: is a value-based Reinforcement Learning algorithm that is used to find the optimal action-selection policy using a Q function.\n",
    "DQN: In deep Q-learning, we use a neural network to approximate the Q-value function. The state is given as the input and the Q-value of allowed actions is the predicted output.\n",
    "\n",
    "\n",
    "\n",
    "https://medium.com/p/f1dad0126a02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at Deep RL paper here:\n",
    "\n",
    "TODO\n",
    "\n",
    "FinRL: A Deep Reinforcement Learning Library for\n",
    " Automated Stock Trading in Quantitative Finance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
