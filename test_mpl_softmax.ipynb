{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## install finrl library\n",
    "# !pip install wrds\n",
    "# !pip install quantstats\n",
    "# !pip install torch_geometric\n",
    "# !pip install swig\n",
    "# !pip install -q condacolab\n",
    "# !pip install shimmy\n",
    "# import condacolab\n",
    "# condacolab.install()\n",
    "# !apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
    "# !pip install git+https://github.com/flpymonkey/FinRL_Online_Portfolio_Benchmarks.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (23286, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bencj\\Desktop\\Econ4\\Thesis\\.venv\\lib\\site-packages\\finrl\\meta\\preprocessor\\preprocessors.py:101: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.31332941 0.43188074 0.33824661 ... 0.02022979 0.02759509 0.02817164]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.loc[select_mask, self.columns] = self.scalers[value].transform(\n",
      "c:\\Users\\bencj\\Desktop\\Econ4\\Thesis\\.venv\\lib\\site-packages\\finrl\\meta\\preprocessor\\preprocessors.py:101: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.5  0.75 1.   ... 0.5  0.75 1.  ]' has dtype incompatible with int32, please explicitly cast to a compatible dtype first.\n",
      "  X.loc[select_mask, self.columns] = self.scalers[value].transform(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BA</th>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTC</th>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V</th>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WMT</th>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "      <td>3212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      date  open  high   low  close  volume   day\n",
       "tic                                              \n",
       "AAPL  3212  3212  3212  3212   3212    3212  3212\n",
       "BA    3212  3212  3212  3212   3212    3212  3212\n",
       "INTC  3212  3212  3212  3212   3212    3212  3212\n",
       "MSFT  3212  3212  3212  3212   3212    3212  3212\n",
       "V     3212  3212  3212  3212   3212    3212  3212\n",
       "WMT   3212  3212  3212  3212   3212    3212  3212"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "\n",
    "from finrl.config import TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "TIME_WINDOW = 25\n",
    "COMMISSION_FEE_PERCENT = 0.001\n",
    "INITIAL_CASH = 1000000\n",
    "\n",
    "\n",
    "TRAIN_START_DATE = '2009-04-01'\n",
    "TRAIN_END_DATE = '2021-12-31'\n",
    "TEST_START_DATE = '2022-01-01'\n",
    "TEST_END_DATE = '2024-09-01'\n",
    "\n",
    "\n",
    "TEST_TICKER = [\n",
    "   \"MSFT\",\n",
    "    \"V\",\n",
    "    \"AAPL\",\n",
    "    \"BA\",\n",
    "    \"INTC\",\n",
    "    \"WMT\",\n",
    "]\n",
    "\n",
    "TRAINED_PPO = \"/agent_opt_ppo_update\"\n",
    "\n",
    "GRAPH_TITLE = \"PPO Trained 2009-2021, test softmax\"\n",
    "\n",
    "\n",
    "\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "\n",
    "\n",
    "\n",
    "df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = TEST_TICKER).fetch_data()\n",
    "\n",
    "processed = df.copy()\n",
    "processed = processed.fillna(0)\n",
    "processed_test = processed.replace(np.inf,0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from finrl.meta.preprocessor.preprocessors import GroupByScaler\n",
    "\n",
    "portfolio_norm_df = GroupByScaler(by=\"tic\", scaler=MaxAbsScaler).fit_transform(processed_test)\n",
    "portfolio_norm_df\n",
    "\n",
    "df_train = portfolio_norm_df[(portfolio_norm_df[\"date\"] >= TRAIN_START_DATE) & (portfolio_norm_df[\"date\"] <= TRAIN_END_DATE)]\n",
    "df_test = portfolio_norm_df[(portfolio_norm_df[\"date\"] >= TEST_START_DATE) & (portfolio_norm_df[\"date\"] < TEST_END_DATE)]\n",
    "\n",
    "# TODO use the start and end date here\n",
    "\n",
    "df_train.groupby(\"tic\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "\n",
    "import gym\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import copy\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import quantstats as qs\n",
    "except ModuleNotFoundError:\n",
    "    raise ModuleNotFoundError(\n",
    "        \"\"\"QuantStats module not found, environment can't plot results and calculate indicadors.\n",
    "        This module is not installed with FinRL. Install by running one of the options:\n",
    "        pip install quantstats --upgrade --no-cache-dir\n",
    "        conda install -c ranaroussi quantstats\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "class PortfolioOptimizationEnv(gym.Env):\n",
    "    \"\"\"A portfolio allocation environment for OpenAI gym.\n",
    "\n",
    "    This environment simulates the interactions between an agent and the financial market\n",
    "    based on data provided by a dataframe. The dataframe contains the time series of\n",
    "    features defined by the user (such as closing, high and low prices) and must have\n",
    "    a time and a tic column with a list of datetimes and ticker symbols respectively.\n",
    "    An example of dataframe is shown below::\n",
    "\n",
    "            date        high            low             close           tic\n",
    "        0   2020-12-23  0.157414        0.127420        0.136394        ADA-USD\n",
    "        1   2020-12-23  34.381519       30.074295       31.097898       BNB-USD\n",
    "        2   2020-12-23  24024.490234    22802.646484    23241.345703    BTC-USD\n",
    "        3   2020-12-23  0.004735        0.003640        0.003768        DOGE-USD\n",
    "        4   2020-12-23  637.122803      560.364258      583.714600      ETH-USD\n",
    "        ... ...         ...             ...             ...             ...\n",
    "\n",
    "    Based on this dataframe, the environment will create an observation space that can\n",
    "    be a Dict or a Box. The Box observation space is a three-dimensional array of shape\n",
    "    (f, n, t), where f is the number of features, n is the number of stocks in the\n",
    "    portfolio and t is the user-defined time window. If the environment is created with\n",
    "    the parameter return_last_action set to True, the observation space is a Dict with\n",
    "    the following keys::\n",
    "\n",
    "        {\n",
    "        \"state\": three-dimensional Box (f, n, t) representing the time series,\n",
    "        \"last_action\": one-dimensional Box (n+1,) representing the portfolio weights\n",
    "        }\n",
    "\n",
    "    Note that the action space of this environment is an one-dimensional Box with size\n",
    "    n + 1 because the portfolio weights must contains the weights related to all the\n",
    "    stocks in the portfolio and to the remaining cash.\n",
    "\n",
    "    Attributes:\n",
    "        action_space: Action space.\n",
    "        observation_space: Observation space.\n",
    "        episode_length: Number of timesteps of an episode.\n",
    "        portfolio_size: Number of stocks in the portfolio.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        initial_amount,\n",
    "        order_df=True,\n",
    "        return_last_action=False,\n",
    "        normalize_df=\"by_previous_time\",\n",
    "        reward_scaling=1,\n",
    "        comission_fee_model=\"trf\",\n",
    "        comission_fee_pct=0,\n",
    "        features=[\"close\", \"high\", \"low\"],\n",
    "        valuation_feature=\"close\",\n",
    "        time_column=\"date\",\n",
    "        time_format=\"%Y-%m-%d\",\n",
    "        tic_column=\"tic\",\n",
    "        tics_in_portfolio=\"all\",\n",
    "        time_window=1,\n",
    "        cwd=\"./\",\n",
    "        new_gym_api=False,\n",
    "        no_cash=False,\n",
    "    ):\n",
    "        \"\"\"Initializes environment's instance.\n",
    "\n",
    "        Args:\n",
    "            df: Dataframe with market information over a period of time.\n",
    "            initial_amount: Initial amount of cash available to be invested.\n",
    "            order_df: If True input dataframe is ordered by time.\n",
    "            return_last_action: If True, observations also return the last performed\n",
    "                action. Note that, in that case, the observation space is a Dict.\n",
    "            normalize_df: Defines the normalization method applied to input dataframe.\n",
    "                Possible values are \"by_previous_time\", \"by_fist_time_window_value\",\n",
    "                \"by_COLUMN_NAME\" (where COLUMN_NAME must be changed to a real column\n",
    "                name) and a custom function. If None no normalization is done.\n",
    "            reward_scaling: A scaling factor to multiply the reward function. This\n",
    "                factor can help training.\n",
    "            comission_fee_model: Model used to simulate comission fee. Possible values\n",
    "                are \"trf\" (for transaction remainder factor model) and \"wvm\" (for weights\n",
    "                vector modifier model). If None, commission fees are not considered.\n",
    "            comission_fee_pct: Percentage to be used in comission fee. It must be a value\n",
    "                between 0 and 1.\n",
    "            features: List of features to be considered in the observation space. The\n",
    "                items of the list must be names of columns of the input dataframe.\n",
    "            valuation_feature: Feature to be considered in the portfolio value calculation.\n",
    "            time_column: Name of the dataframe's column that contain the datetimes that\n",
    "                index the dataframe.\n",
    "            time_format: Formatting string of time column.\n",
    "            tic_name: Name of the dataframe's column that contain ticker symbols.\n",
    "            tics_in_portfolio: List of ticker symbols to be considered as part of the\n",
    "                portfolio. If \"all\", all tickers of input data are considered.\n",
    "            time_window: Size of time window.\n",
    "            cwd: Local repository in which resulting graphs will be saved.\n",
    "            new_gym_api: If True, the environment will use the new gym api standard for\n",
    "                step and reset methods.\n",
    "        \"\"\"\n",
    "        self._time_window = time_window\n",
    "        self._time_index = time_window - 1\n",
    "        self._time_column = time_column\n",
    "        self._time_format = time_format\n",
    "        self._tic_column = tic_column\n",
    "        self._df = df\n",
    "        self._initial_amount = initial_amount\n",
    "        self._return_last_action = return_last_action\n",
    "        self._reward_scaling = reward_scaling\n",
    "        self._comission_fee_pct = comission_fee_pct\n",
    "        self._comission_fee_model = comission_fee_model\n",
    "        self._features = features\n",
    "        self._valuation_feature = valuation_feature\n",
    "        self._cwd = Path(cwd)\n",
    "        self._new_gym_api = new_gym_api\n",
    "        self._no_cash = no_cash\n",
    "\n",
    "        # results file\n",
    "        self._results_file = self._cwd / \"results\" / \"rl\"\n",
    "        self._results_file.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # initialize price variation\n",
    "        self._df_price_variation = None\n",
    "\n",
    "        # preprocess data\n",
    "        self._preprocess_data(order_df, normalize_df, tics_in_portfolio)\n",
    "\n",
    "        # dims and spaces\n",
    "        self._tic_list = self._df[self._tic_column].unique()\n",
    "        self.portfolio_size = (\n",
    "            len(self._tic_list)\n",
    "            if tics_in_portfolio == \"all\"\n",
    "            else len(tics_in_portfolio)\n",
    "        )\n",
    "        action_space = 1 + self.portfolio_size\n",
    "\n",
    "        # sort datetimes and define episode length\n",
    "        self._sorted_times = sorted(set(self._df[time_column]))\n",
    "        self.episode_length = len(self._sorted_times) - time_window + 1\n",
    "\n",
    "        # define action space\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(action_space,))\n",
    "\n",
    "        # define observation state\n",
    "        if self._return_last_action:\n",
    "            # if  last action must be returned, a dict observation\n",
    "            # is defined\n",
    "            self.observation_space = spaces.Dict(\n",
    "                {\n",
    "                    \"state\": spaces.Box(\n",
    "                        low=-np.inf,\n",
    "                        high=np.inf,\n",
    "                        shape=(\n",
    "                            len(self._features),\n",
    "                            len(self._tic_list),\n",
    "                            self._time_window,\n",
    "                        ),\n",
    "                    ),\n",
    "                    \"last_action\": spaces.Box(low=0, high=1, shape=(action_space,)),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            # if information about last action is not relevant,\n",
    "            # a 3D observation space is defined\n",
    "            self.observation_space = spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                shape=(len(self._features), len(self._tic_list), self._time_window),\n",
    "            )\n",
    "\n",
    "        self._reset_memory()\n",
    "\n",
    "        self._total_transaction_cost = 0\n",
    "        self._portfolio_value = self._initial_amount\n",
    "        self._terminal = False\n",
    "\n",
    "        # Use this to save the state in the last terminal state in case the environment resets \n",
    "        self._terminal_action_memory = None\n",
    "        self._terminal_asset_memory = None\n",
    "        self._terminal_date_memory = None\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"Performs a simulation step.\n",
    "\n",
    "        Args:\n",
    "            actions: An unidimensional array containing the new portfolio\n",
    "                weights.\n",
    "\n",
    "        Note:\n",
    "            If the environment was created with \"return_last_action\" set to\n",
    "            True, the next state returned will be a Dict. If it's set to False,\n",
    "            the next state will be a Box. You can check the observation state\n",
    "            through the attribute \"observation_space\".\n",
    "\n",
    "        Returns:\n",
    "            If \"new_gym_api\" is set to True, the following tuple is returned:\n",
    "            (state, reward, terminal, truncated, info). If it's set to False,\n",
    "            the following tuple is returned: (state, reward, terminal, info).\n",
    "\n",
    "            state: Next simulation state.\n",
    "            reward: Reward related to the last performed action.\n",
    "            terminal: If True, the environment is in a terminal state.\n",
    "            truncated: If True, the environment has passed it's simulation\n",
    "                time limit. Currently, it's always False.\n",
    "            info: A dictionary containing informations about the last state.\n",
    "        \"\"\"\n",
    "        self._terminal = self._time_index >= len(self._sorted_times) - 1\n",
    "\n",
    "        if self._terminal:\n",
    "            metrics_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"date\": self._date_memory,\n",
    "                    \"returns\": self._portfolio_return_memory,\n",
    "                    \"rewards\": self._portfolio_reward_memory,\n",
    "                    \"portfolio_values\": self._asset_memory[\"final\"],\n",
    "                }\n",
    "            )\n",
    "            metrics_df.set_index(\"date\", inplace=True)\n",
    "\n",
    "            plt.plot(metrics_df[\"portfolio_values\"], \"r\")\n",
    "            plt.title(\"Portfolio Value Over Time\")\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Portfolio value\")\n",
    "            plt.savefig(self._results_file / \"portfolio_value.png\")\n",
    "            plt.close()\n",
    "\n",
    "            plt.plot(self._portfolio_reward_memory, \"r\")\n",
    "            plt.title(\"Reward Over Time\")\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.savefig(self._results_file / \"reward.png\")\n",
    "            plt.close()\n",
    "\n",
    "            plt.plot(self._actions_memory)\n",
    "            plt.title(\"Actions performed\")\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Weight\")\n",
    "            plt.savefig(self._results_file / \"actions.png\")\n",
    "            plt.close()\n",
    "\n",
    "            print(\"=================================\")\n",
    "            print(\"Initial portfolio value:{}\".format(self._asset_memory[\"final\"][0]))\n",
    "            print(f\"Final portfolio value: {self._portfolio_value}\")\n",
    "            print(\n",
    "                \"Final accumulative portfolio value: {}\".format(\n",
    "                    self._portfolio_value / self._asset_memory[\"final\"][0]\n",
    "                )\n",
    "            )\n",
    "            print(\n",
    "                \"Maximum DrawDown: {}\".format(\n",
    "                    qs.stats.max_drawdown(metrics_df[\"portfolio_values\"])\n",
    "                )\n",
    "            )\n",
    "            print(\"Sharpe ratio: {}\".format(qs.stats.sharpe(metrics_df[\"returns\"])))\n",
    "            print(\"Total commission cost: {}\".format(self._total_transaction_cost))\n",
    "            print(\"=================================\")\n",
    "\n",
    "            qs.plots.snapshot(\n",
    "                metrics_df[\"returns\"],\n",
    "                show=False,\n",
    "                savefig=self._results_file / \"portfolio_summary.png\",\n",
    "            )\n",
    "\n",
    "            # Save the asset memory in the terminal state before the environment is reset\n",
    "            self._terminal_asset_memory = copy.deepcopy(self._asset_memory)\n",
    "            self._terminal_date_memory = copy.deepcopy(self._date_memory)\n",
    "            self._terminal_action_memory = copy.deepcopy(self._actions_memory)\n",
    "\n",
    "            file_path = './action_dump/actions' + str(len(self._terminal_action_memory)) + '.csv'\n",
    "            np.savetxt(file_path, self._terminal_action_memory, delimiter=',', fmt='%.6f')\n",
    "\n",
    "            if self._new_gym_api:\n",
    "                return self._state, self._reward, self._terminal, False, self._info\n",
    "            return self._state, self._reward, self._terminal, self._info\n",
    "\n",
    "        else:\n",
    "            # transform action to numpy array (if it's a list)\n",
    "            actions = np.array(actions, dtype=np.float32)\n",
    "\n",
    "            # if necessary, normalize weights\n",
    "            if math.isclose(np.sum(actions), 1, abs_tol=1e-6) and np.min(actions) >= 0:\n",
    "                weights = actions\n",
    "            else:\n",
    "                assert False, \"THIS SHOULD NOT BE HIT\"\n",
    "                weights = self._softmax_normalization(actions)\n",
    "\n",
    "            # save initial portfolio weights for this time step\n",
    "            self._actions_memory.append(weights)\n",
    "\n",
    "            # get last step final weights and portfolio_value\n",
    "            last_weights = self._final_weights[-1]\n",
    "\n",
    "            # load next state\n",
    "            self._time_index += 1\n",
    "            self._state, self._info = self._get_state_and_info_from_time_index(\n",
    "                self._time_index\n",
    "            )\n",
    "\n",
    "            # if using weights vector modifier, we need to modify weights vector\n",
    "            if self._comission_fee_model == \"wvm\":\n",
    "                delta_weights = weights - last_weights\n",
    "                delta_assets = delta_weights[1:]  # disconsider\n",
    "                # calculate fees considering weights modification\n",
    "                fees = np.sum(np.abs(delta_assets * self._portfolio_value))\n",
    "                if fees > weights[0] * self._portfolio_value:\n",
    "                    weights = last_weights\n",
    "                    # maybe add negative reward\n",
    "                else:\n",
    "                    portfolio = weights * self._portfolio_value\n",
    "                    portfolio[0] -= fees\n",
    "                    self._portfolio_value = np.sum(portfolio)  # new portfolio value\n",
    "                    weights = portfolio / self._portfolio_value  # new weights\n",
    "\n",
    "                    self._total_transaction_cost += fees\n",
    "            elif self._comission_fee_model == \"trf\":\n",
    "                last_mu = 1\n",
    "                mu = 1 - 2 * self._comission_fee_pct + self._comission_fee_pct**2\n",
    "                while abs(mu - last_mu) > 1e-10:\n",
    "                    last_mu = mu\n",
    "                    mu = (\n",
    "                        1\n",
    "                        - self._comission_fee_pct * weights[0]\n",
    "                        - (2 * self._comission_fee_pct - self._comission_fee_pct**2)\n",
    "                        * np.sum(np.maximum(last_weights[1:] - mu * weights[1:], 0))\n",
    "                    ) / (1 - self._comission_fee_pct * weights[0])\n",
    "                self._info[\"trf_mu\"] = mu\n",
    "\n",
    "                self._total_transaction_cost += (self._portfolio_value - mu * self._portfolio_value)\n",
    "\n",
    "                self._portfolio_value = mu * self._portfolio_value\n",
    "\n",
    "            # save initial portfolio value of this time step\n",
    "            self._asset_memory[\"initial\"].append(self._portfolio_value)\n",
    "\n",
    "            # time passes and time variation changes the portfolio distribution\n",
    "            portfolio = self._portfolio_value * (weights * self._price_variation)\n",
    "\n",
    "            # calculate new portfolio value and weights\n",
    "            self._portfolio_value = np.sum(portfolio)\n",
    "            weights = portfolio / self._portfolio_value\n",
    "\n",
    "            # save final portfolio value and weights of this time step\n",
    "            self._asset_memory[\"final\"].append(self._portfolio_value)\n",
    "            self._final_weights.append(weights)\n",
    "\n",
    "            # save date memory\n",
    "            self._date_memory.append(self._info[\"end_time\"])\n",
    "\n",
    "            # define portfolio return\n",
    "            rate_of_return = (\n",
    "                self._asset_memory[\"final\"][-1] / self._asset_memory[\"final\"][-2]\n",
    "            )\n",
    "            portfolio_return = rate_of_return - 1\n",
    "            portfolio_reward = np.log(rate_of_return)\n",
    "\n",
    "            # save portfolio return memory\n",
    "            self._portfolio_return_memory.append(portfolio_return)\n",
    "            self._portfolio_reward_memory.append(portfolio_reward)\n",
    "\n",
    "            # Define portfolio return\n",
    "            self._reward = portfolio_reward\n",
    "            self._reward = self._reward * self._reward_scaling\n",
    "        \n",
    "        if self._new_gym_api:\n",
    "            return self._state, self._reward, self._terminal, False, self._info\n",
    "        return self._state, self._reward, self._terminal, self._info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment and returns it to its initial state (the\n",
    "        fist date of the dataframe).\n",
    "\n",
    "        Note:\n",
    "            If the environment was created with \"return_last_action\" set to\n",
    "            True, the initial state will be a Dict. If it's set to False,\n",
    "            the initial state will be a Box. You can check the observation\n",
    "            state through the attribute \"observation_space\".\n",
    "\n",
    "        Returns:\n",
    "            If \"new_gym_api\" is set to True, the following tuple is returned:\n",
    "            (state, info). If it's set to False, only the initial state is\n",
    "            returned.\n",
    "\n",
    "            state: Initial state.\n",
    "            info: Initial state info.\n",
    "        \"\"\"\n",
    "        # time_index must start a little bit in the future to implement lookback\n",
    "        self._time_index = self._time_window - 1\n",
    "        self._reset_memory()\n",
    "\n",
    "        self._state, self._info = self._get_state_and_info_from_time_index(\n",
    "            self._time_index\n",
    "        )\n",
    "        self._portfolio_value = self._initial_amount\n",
    "        self._terminal = False\n",
    "\n",
    "        if self._new_gym_api:\n",
    "            return self._state, self._info\n",
    "        return self._state\n",
    "\n",
    "    def _get_state_and_info_from_time_index(self, time_index):\n",
    "        \"\"\"Gets state and information given a time index. It also updates \"data\"\n",
    "        attribute with information about the current simulation step.\n",
    "\n",
    "        Args:\n",
    "            time_index: An integer that represents the index of a specific datetime.\n",
    "                The initial datetime of the dataframe is given by 0.\n",
    "\n",
    "        Note:\n",
    "            If the environment was created with \"return_last_action\" set to\n",
    "            True, the returned state will be a Dict. If it's set to False,\n",
    "            the returned state will be a Box. You can check the observation\n",
    "            state through the attribute \"observation_space\".\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the following form: (state, info).\n",
    "\n",
    "            state: The state of the current time index. It can be a Box or a Dict.\n",
    "            info: A dictionary with some informations about the current simulation\n",
    "                step. The dict has the following keys::\n",
    "\n",
    "                {\n",
    "                \"tics\": List of ticker symbols,\n",
    "                \"start_time\": Start time of current time window,\n",
    "                \"start_time_index\": Index of start time of current time window,\n",
    "                \"end_time\": End time of current time window,\n",
    "                \"end_time_index\": Index of end time of current time window,\n",
    "                \"data\": Data related to the current time window,\n",
    "                \"price_variation\": Price variation of current time step\n",
    "                }\n",
    "        \"\"\"\n",
    "        # returns state in form (channels, tics, timesteps)\n",
    "        end_time = self._sorted_times[time_index]\n",
    "        start_time = self._sorted_times[time_index - (self._time_window - 1)]\n",
    "\n",
    "        # define data to be used in this time step\n",
    "        self._data = self._df[\n",
    "            (self._df[self._time_column] >= start_time)\n",
    "            & (self._df[self._time_column] <= end_time)\n",
    "        ][[self._time_column, self._tic_column] + self._features]\n",
    "\n",
    "        # define price variation of this time_step\n",
    "        self._price_variation = self._df_price_variation[\n",
    "            self._df_price_variation[self._time_column] == end_time\n",
    "        ][self._valuation_feature].to_numpy()\n",
    "        self._price_variation = np.insert(self._price_variation, 0, 1)\n",
    "\n",
    "        # define state to be returned\n",
    "        state = None\n",
    "        for tic in self._tic_list:\n",
    "            tic_data = self._data[self._data[self._tic_column] == tic]\n",
    "            tic_data = tic_data[self._features].to_numpy().T\n",
    "            tic_data = tic_data[..., np.newaxis]\n",
    "            state = tic_data if state is None else np.append(state, tic_data, axis=2)\n",
    "        state = state.transpose((0, 2, 1))\n",
    "        info = {\n",
    "            \"tics\": self._tic_list,\n",
    "            \"start_time\": start_time,\n",
    "            \"start_time_index\": time_index - (self._time_window - 1),\n",
    "            \"end_time\": end_time,\n",
    "            \"end_time_index\": time_index,\n",
    "            \"data\": self._data,\n",
    "            \"price_variation\": self._price_variation,\n",
    "        }\n",
    "        return self._standardize_state(state), info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"Renders the environment.\n",
    "\n",
    "        Returns:\n",
    "            Observation of current simulation step.\n",
    "        \"\"\"\n",
    "        return self._state\n",
    "\n",
    "    def _softmax_normalization(self, actions):\n",
    "        \"\"\"Normalizes the action vector using softmax function.\n",
    "\n",
    "        Returns:\n",
    "            Normalized action vector (portfolio vector).\n",
    "        \"\"\"\n",
    "        numerator = np.exp(actions)\n",
    "        denominator = np.sum(np.exp(actions))\n",
    "        softmax_output = numerator / denominator\n",
    "        return softmax_output\n",
    "\n",
    "    def enumerate_portfolio(self):\n",
    "        \"\"\"Enumerates the current porfolio by showing the ticker symbols\n",
    "        of all the investments considered in the portfolio.\n",
    "        \"\"\"\n",
    "        print(\"Index: 0. Tic: Cash\")\n",
    "        for index, tic in enumerate(self._tic_list):\n",
    "            print(f\"Index: {index + 1}. Tic: {tic}\")\n",
    "\n",
    "    def _preprocess_data(self, order, normalize, tics_in_portfolio):\n",
    "        \"\"\"Orders and normalizes the environment's dataframe.\n",
    "\n",
    "        Args:\n",
    "            order: If true, the dataframe will be ordered by ticker list\n",
    "                and datetime.\n",
    "            normalize: Defines the normalization method applied to the dataframe.\n",
    "                Possible values are \"by_previous_time\", \"by_fist_time_window_value\",\n",
    "                \"by_COLUMN_NAME\" (where COLUMN_NAME must be changed to a real column\n",
    "                name) and a custom function. If None no normalization is done.\n",
    "            tics_in_portfolio: List of ticker symbols to be considered as part of the\n",
    "                portfolio. If \"all\", all tickers of input data are considered.\n",
    "        \"\"\"\n",
    "        # order time dataframe by tic and time\n",
    "        if order:\n",
    "            self._df = self._df.sort_values(by=[self._tic_column, self._time_column])\n",
    "        # defining price variation after ordering dataframe\n",
    "        self._df_price_variation = self._temporal_variation_df()\n",
    "        # select only stocks in portfolio\n",
    "        if tics_in_portfolio != \"all\":\n",
    "            self._df_price_variation = self._df_price_variation[\n",
    "                self._df_price_variation[self._tic_column].isin(tics_in_portfolio)\n",
    "            ]\n",
    "        # apply normalization\n",
    "        if normalize:\n",
    "            self._normalize_dataframe(normalize)\n",
    "        # transform str to datetime\n",
    "        self._df[self._time_column] = pd.to_datetime(self._df[self._time_column])\n",
    "        self._df_price_variation[self._time_column] = pd.to_datetime(\n",
    "            self._df_price_variation[self._time_column]\n",
    "        )\n",
    "        # transform numeric variables to float32 (compatibility with pytorch)\n",
    "        self._df[self._features] = self._df[self._features].astype(\"float32\")\n",
    "        self._df_price_variation[self._features] = self._df_price_variation[\n",
    "            self._features\n",
    "        ].astype(\"float32\")\n",
    "\n",
    "    def _reset_memory(self):\n",
    "        \"\"\"Resets the environment's memory.\"\"\"\n",
    "        date_time = self._sorted_times[self._time_index]\n",
    "        # memorize portfolio value each step\n",
    "        self._asset_memory = {\n",
    "            \"initial\": [self._initial_amount],\n",
    "            \"final\": [self._initial_amount],\n",
    "        }\n",
    "        # memorize portfolio return and reward each step\n",
    "        self._portfolio_return_memory = [0]\n",
    "        self._portfolio_reward_memory = [0]\n",
    "        # initial action: all money is allocated in cash\n",
    "        self._actions_memory = [\n",
    "            np.array([1] + [0] * self.portfolio_size, dtype=np.float32)\n",
    "        ]\n",
    "        # memorize portfolio weights at the ending of time step\n",
    "        self._final_weights = [\n",
    "            np.array([1] + [0] * self.portfolio_size, dtype=np.float32)\n",
    "        ]\n",
    "        # memorize datetimes\n",
    "        self._date_memory = [date_time]\n",
    "\n",
    "    def _standardize_state(self, state):\n",
    "        \"\"\"Standardize the state given the observation space. If \"return_last_action\"\n",
    "        is set to False, a three-dimensional box is returned. If it's set to True, a\n",
    "        dictionary is returned. The dictionary follows the standard below::\n",
    "\n",
    "            {\n",
    "            \"state\": Three-dimensional box representing the current state,\n",
    "            \"last_action\": One-dimensional box representing the last action\n",
    "            }\n",
    "        \"\"\"\n",
    "        last_action = self._actions_memory[-1]\n",
    "        if self._return_last_action:\n",
    "            return {\"state\": state, \"last_action\": last_action}\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "    def _normalize_dataframe(self, normalize):\n",
    "        \"\"\" \"Normalizes the environment's dataframe.\n",
    "\n",
    "        Args:\n",
    "            normalize: Defines the normalization method applied to the dataframe.\n",
    "                Possible values are \"by_previous_time\", \"by_fist_time_window_value\",\n",
    "                \"by_COLUMN_NAME\" (where COLUMN_NAME must be changed to a real column\n",
    "                name) and a custom function. If None no normalization is done.\n",
    "\n",
    "        Note:\n",
    "            If a custom function is used in the normalization, it must have an\n",
    "            argument representing the environment's dataframe.\n",
    "        \"\"\"\n",
    "        if type(normalize) == str:\n",
    "            if normalize == \"by_fist_time_window_value\":\n",
    "                print(\n",
    "                    \"Normalizing {} by first time window value...\".format(\n",
    "                        self._features\n",
    "                    )\n",
    "                )\n",
    "                self._df = self._temporal_variation_df(self._time_window - 1)\n",
    "            elif normalize == \"by_previous_time\":\n",
    "                print(f\"Normalizing {self._features} by previous time...\")\n",
    "                self._df = self._temporal_variation_df()\n",
    "            elif normalize.startswith(\"by_\"):\n",
    "                normalizer_column = normalize[3:]\n",
    "                print(f\"Normalizing {self._features} by {normalizer_column}\")\n",
    "                for column in self._features:\n",
    "                    self._df[column] = self._df[column] / self._df[normalizer_column]\n",
    "        elif callable(normalize):\n",
    "            print(\"Applying custom normalization function...\")\n",
    "            self._df = normalize(self._df)\n",
    "        else:\n",
    "            print(\"No normalization was performed.\")\n",
    "\n",
    "    def _temporal_variation_df(self, periods=1):\n",
    "        \"\"\"Calculates the temporal variation dataframe. For each feature, this\n",
    "        dataframe contains the rate of the current feature's value and the last\n",
    "        feature's value given a period. It's used to normalize the dataframe.\n",
    "\n",
    "        Args:\n",
    "            periods: Periods (in time indexes) to calculate temporal variation.\n",
    "\n",
    "        Returns:\n",
    "            Temporal variation dataframe.\n",
    "        \"\"\"\n",
    "        df_temporal_variation = self._df.copy()\n",
    "        prev_columns = []\n",
    "        for column in self._features:\n",
    "            prev_column = f\"prev_{column}\"\n",
    "            prev_columns.append(prev_column)\n",
    "            df_temporal_variation[prev_column] = df_temporal_variation.groupby(\n",
    "                self._tic_column\n",
    "            )[column].shift(periods=periods)\n",
    "            df_temporal_variation[column] = (\n",
    "                df_temporal_variation[column] / df_temporal_variation[prev_column]\n",
    "            )\n",
    "        df_temporal_variation = (\n",
    "            df_temporal_variation.drop(columns=prev_columns)\n",
    "            .fillna(1)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        return df_temporal_variation\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        \"\"\"Seeds the sources of randomness of this environment to guarantee\n",
    "        reproducibility.\n",
    "\n",
    "        Args:\n",
    "            seed: Seed value to be applied.\n",
    "\n",
    "        Returns:\n",
    "            Seed value applied.\n",
    "        \"\"\"\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_sb_env(self, env_number=1):\n",
    "        \"\"\"Generates an environment compatible with Stable Baselines 3. The\n",
    "        generated environment is a vectorized version of the current one.\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the generated environment and an initial observation.\n",
    "        \"\"\"\n",
    "        e = DummyVecEnv([lambda: self] * env_number)\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit for the portfolio optimization model\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from finrl.meta.preprocessor.preprocessors import GroupByScaler\n",
    "\n",
    "from finrl.meta.preprocessor.preprocessors import data_split\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "environment_ppo = PortfolioOptimizationEnv(\n",
    "    df_train,\n",
    "    initial_amount=INITIAL_CASH,\n",
    "    comission_fee_pct=COMMISSION_FEE_PERCENT,\n",
    "    # time_window=TIME_WINDOW,\n",
    "    features=[\"close\", \"high\", \"low\"],\n",
    "    normalize_df=None,\n",
    "    reward_scaling=1e-4,\n",
    "    no_cash=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to results/ppo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bencj\\Desktop\\Econ4\\Thesis\\.venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from finrl.agents.portfolio_optimization.models_stable import DRLStableAgent, CustomActorCriticPolicy\n",
    "\n",
    "\n",
    "\n",
    "agent_ppo = DRLStableAgent(env = environment_ppo)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"batch_size\": 64,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025, # TODO tried raising the lr which caused vanishing problem\n",
    "    \"clip_range\": 0.1,\n",
    "    # \"gae_lambda\": 0.001,\n",
    "}\n",
    "\n",
    "# Lower clip_range makes the stocks flatter, very conservative policy\n",
    "\n",
    "# TODO try playing around with the number of epochs? n_epochs\n",
    "# TODO try playing around more with the entropy term, make sure agent does enough exploration during training\n",
    "# TODO try playing around more with the clip papram here\n",
    "\n",
    "\n",
    "model_ppo = agent_ppo.get_model(\"ppo\", device, model_kwargs=PPO_PARAMS, policy_kwargs=None)\n",
    "\n",
    "# set up logger\n",
    "tmp_path = RESULTS_DIR + '/ppo'\n",
    "new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "# Set new logger\n",
    "model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780600ce08d0453994b77cf0a77b0722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of time steps in an episode:  3212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 105  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 19   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">=================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "=================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initial portfolio value:1000000\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initial portfolio value:1000000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Final portfolio value: 536845.375\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Final portfolio value: 536845.375\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Final accumulative portfolio value: 0.536845375\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Final accumulative portfolio value: 0.536845375\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Maximum DrawDown: -0.6085114629390982\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Maximum DrawDown: -0.6085114629390982\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sharpe ratio: -0.17687445099466703\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Sharpe ratio: -0.17687445099466703\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total commission cost: 2294513.0914263865\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total commission cost: 2294513.0914263865\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">=================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "=================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 3.21e+03 |\n",
      "|    ep_rew_mean          | -6.2e-05 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 97       |\n",
      "|    iterations           | 2        |\n",
      "|    time_elapsed         | 41       |\n",
      "|    total_timesteps      | 4096     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 6.256488 |\n",
      "|    clip_fraction        | 0.994    |\n",
      "|    clip_range           | 0.1      |\n",
      "|    entropy_loss         | -10      |\n",
      "|    explained_variance   | 0.87     |\n",
      "|    learning_rate        | 0.00025  |\n",
      "|    loss                 | 0.22     |\n",
      "|    n_updates            | 10       |\n",
      "|    policy_gradient_loss | 65.6     |\n",
      "|    std                  | 1.02     |\n",
      "|    value_loss           | 1.94e-05 |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ppo = DRLStableAgent.train_model(model_ppo, env=environment_ppo, episodes=1)\n",
    "\n",
    "from finrl.config import TRAINED_MODEL_DIR\n",
    "\n",
    "environment_ppo.reset()\n",
    "\n",
    "model_ppo.save(TRAINED_MODEL_DIR + \"/agent_opt_ppo_update\", include=[\"policy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from finrl.config import TRAINED_MODEL_DIR\n",
    "\n",
    "\n",
    "# Load the trained models\n",
    "# trained_ppo_opt = PPO.load(TRAINED_MODEL_DIR + \"/agent_opt_ppo_10_27\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trained_ppo_opt = PPO.load(TRAINED_MODEL_DIR + TRAINED_PPO, policy=CustomActorCriticPolicy) \n",
    "\n",
    "\n",
    "# trained_ppo_opt = model_ppo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "Observations: \n",
      "[[[[0.01397609]\n",
      "   [0.0624001 ]\n",
      "   [0.1531402 ]\n",
      "   [0.03070461]\n",
      "   [0.0420142 ]\n",
      "   [0.16277683]]\n",
      "\n",
      "  [[0.01640963]\n",
      "   [0.07981883]\n",
      "   [0.22109972]\n",
      "   [0.04133661]\n",
      "   [0.04811658]\n",
      "   [0.22826038]]\n",
      "\n",
      "  [[0.01591813]\n",
      "   [0.07771644]\n",
      "   [0.21720399]\n",
      "   [0.03914223]\n",
      "   [0.04656999]\n",
      "   [0.2240007 ]]]]\n",
      "Actions: \n",
      "[[0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bencj\\Desktop\\Econ4\\Thesis\\.venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "THIS SHOULD NOT BE HIT",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m agent_ppo_test \u001b[38;5;241m=\u001b[39m DRLStableAgent(env \u001b[38;5;241m=\u001b[39m environment_ppo)\n\u001b[0;32m      3\u001b[0m PPO_results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[0;32m      5\u001b[0m }\n\u001b[1;32m----> 7\u001b[0m values, dates \u001b[38;5;241m=\u001b[39m \u001b[43mDRLStableAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDRL_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_ppo_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment_ppo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m PPO_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m environment_ppo\u001b[38;5;241m.\u001b[39m_terminal_asset_memory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m PPO_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m environment_ppo\u001b[38;5;241m.\u001b[39m_terminal_date_memory\n",
      "File \u001b[1;32mc:\\Users\\bencj\\Desktop\\Econ4\\Thesis\\.venv\\lib\\site-packages\\finrl\\agents\\portfolio_optimization\\models_stable.py:302\u001b[0m, in \u001b[0;36mDRLStableAgent.DRL_prediction\u001b[1;34m(model, env, deterministic, verbose)\u001b[0m\n\u001b[0;32m    299\u001b[0m validation_assets \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39m_asset_memory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    300\u001b[0m validation_dates \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39m_date_memory\n\u001b[1;32m--> 302\u001b[0m test_obs, rewards, dones, info \u001b[38;5;241m=\u001b[39m \u001b[43mtest_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dones[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhit end!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bencj\\Desktop\\Econ4\\Thesis\\.venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bencj\\Desktop\\Econ4\\Thesis\\.venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\bencj\\Desktop\\Econ4\\Thesis\\.venv\\lib\\site-packages\\shimmy\\openai_gym_compatibility.py:251\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "Cell \u001b[1;32mIn[22], line 309\u001b[0m, in \u001b[0;36mPortfolioOptimizationEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    307\u001b[0m     weights \u001b[38;5;241m=\u001b[39m actions\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTHIS SHOULD NOT BE HIT\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_softmax_normalization(actions)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# save initial portfolio weights for this time step\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: THIS SHOULD NOT BE HIT"
     ]
    }
   ],
   "source": [
    "agent_ppo_test = DRLStableAgent(env = environment_ppo)\n",
    "\n",
    "PPO_results = {\n",
    "    \"train\": {},\n",
    "}\n",
    "\n",
    "values, dates = DRLStableAgent.DRL_prediction(trained_ppo_opt, environment_ppo, verbose=True)\n",
    "PPO_results[\"train\"][\"value\"] = environment_ppo._terminal_asset_memory[\"final\"]\n",
    "PPO_results[\"train\"][\"date\"] = environment_ppo._terminal_date_memory\n",
    "\n",
    "\n",
    "# Write this out to a csv file, with date and net worth\n",
    "df_ppo_opt = pd.DataFrame(PPO_results[\"train\"][\"value\"], columns=['ppo_opt_net_worth'])\n",
    "df_ppo_date = pd.DataFrame(PPO_results[\"train\"][\"date\"], columns=['Date'])\n",
    "if len(df_ppo_opt) == len(df_ppo_date):\n",
    "    df_ppo_opt['Date'] = df_ppo_date['Date']\n",
    "else:\n",
    "    raise ValueError(\"DataFrames do not have the same number of rows.\")\n",
    "\n",
    "print(df_ppo_opt)\n",
    "\n",
    "\n",
    "print(df_ppo_opt.loc[0, 'Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bencj\\Desktop\\Econ4\\Thesis\\.venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1092445.125\n",
      "Final accumulative portfolio value: 1.092445125\n",
      "Maximum DrawDown: -0.250551375\n",
      "Sharpe ratio: 0.2766945442210661\n",
      "Total commission cost: 5374.11833590013\n",
      "=================================\n",
      "hit end!\n",
      "     ppo_opt_net_worth       Date\n",
      "0         1.000000e+06 2022-01-03\n",
      "1         9.983872e+05 2022-01-04\n",
      "2         9.910549e+05 2022-01-05\n",
      "3         9.862322e+05 2022-01-06\n",
      "4         9.874698e+05 2022-01-07\n",
      "..                 ...        ...\n",
      "664       1.069093e+06 2024-08-26\n",
      "665       1.070488e+06 2024-08-27\n",
      "666       1.061776e+06 2024-08-28\n",
      "667       1.073613e+06 2024-08-29\n",
      "668       1.092445e+06 2024-08-30\n",
      "\n",
      "[669 rows x 2 columns]\n",
      "2022-01-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "environment_ppo_test = PortfolioOptimizationEnv(\n",
    "    df_test,\n",
    "    initial_amount=INITIAL_CASH,\n",
    "    comission_fee_pct=COMMISSION_FEE_PERCENT,\n",
    "    # time_window=TIME_WINDOW,\n",
    "    features=[\"close\", \"high\", \"low\"],\n",
    "    normalize_df=None,\n",
    "    reward_scaling=1e-4,\n",
    "    no_cash=True\n",
    ")\n",
    "\n",
    "PPO_results = {\n",
    "    \"test\": {},\n",
    "}\n",
    "\n",
    "values, dates = DRLStableAgent.DRL_prediction(trained_ppo_opt, environment_ppo_test, verbose=False)\n",
    "PPO_results[\"test\"][\"value\"] = environment_ppo_test._terminal_asset_memory[\"final\"]\n",
    "PPO_results[\"test\"][\"date\"] = environment_ppo_test._terminal_date_memory\n",
    "\n",
    "\n",
    "# Write this out to a csv file, with date and net worth\n",
    "df_ppo_opt = pd.DataFrame(PPO_results[\"test\"][\"value\"], columns=['ppo_opt_net_worth'])\n",
    "df_ppo_date = pd.DataFrame(PPO_results[\"test\"][\"date\"], columns=['Date'])\n",
    "if len(df_ppo_opt) == len(df_ppo_date):\n",
    "    df_ppo_opt['Date'] = df_ppo_date['Date']\n",
    "else:\n",
    "    raise ValueError(\"DataFrames do not have the same number of rows.\")\n",
    "\n",
    "print(df_ppo_opt)\n",
    "\n",
    "\n",
    "print(df_ppo_opt.loc[0, 'Date'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
